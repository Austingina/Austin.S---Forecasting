{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86ac496",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "\n",
    "# ---------------------- 1. æ ¸å¿ƒé…ç½®ï¼ˆè¯·æ ¹æ®å®é™…æƒ…å†µä¿®æ”¹ï¼‰ ----------------------\n",
    "class FastConfig:\n",
    "    def __init__(self):\n",
    "        # 1. å·²è®­ç»ƒæ¨¡å‹/è¯å…¸è·¯å¾„ï¼ˆè®­ç»ƒç»“æœæ–‡ä»¶å¤¹ï¼‰\n",
    "        self.TRAINED_FOLDER = \"financial_bert_results\"\n",
    "        self.MODEL_WEIGHT = os.path.join(self.TRAINED_FOLDER, \"financial_bert_best_model.pth\")\n",
    "        self.SENTIMENT_DICT = os.path.join(self.TRAINED_FOLDER, \"financial_bert_sentiment_dict.json\")\n",
    "        \n",
    "        # 2. æœ¬åœ°BERTåŸºç¡€æ¨¡å‹è·¯å¾„ï¼ˆä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼‰\n",
    "        self.LOCAL_BERT = r\"hf_cach\"\n",
    "        \n",
    "        # 3. è¾“å…¥æ•°æ®è·¯å¾„\n",
    "        self.BAR_DATA = \"ç‰ç±³_è‚¡å§æ•°æ®_2020-20244.xlsx\"  # è‚¡å§æ•°æ®\n",
    "        self.STOCK_DATA = \"ç‰ç±³.csv\"  # è‚¡ç¥¨æ•°æ®\n",
    "        self.STOPWORDS = \"stopwords.txt\"  # åœç”¨è¯è¡¨\n",
    "        \n",
    "        # 4. è¾“å‡ºè·¯å¾„\n",
    "        self.OUTPUT = \"ç‰ç±³_å¸¦æƒ…æ„Ÿå¾—åˆ†.csv\"\n",
    "        \n",
    "        # 5. æ¨¡å‹å‚æ•°\n",
    "        self.BATCH_SIZE = 16  # CPUå»ºè®®8ï¼ŒGPUå»ºè®®16-32\n",
    "        self.MAX_LEN = 128    # æ–‡æœ¬æœ€å¤§é•¿åº¦\n",
    "        self.DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        # 6. æ—¥æœŸåˆ—é…ç½® - è¯·æ ¹æ®ä½ çš„è‚¡ç¥¨æ•°æ®ä¿®æ”¹ï¼\n",
    "        self.STOCK_DATE_COLUMN = \"date\"  # è‚¡ç¥¨æ•°æ®ä¸­çš„æ—¥æœŸåˆ—åç§°\n",
    "        self.BAR_DATE_COLUMN = \"æœ€åæ›´æ–°æ—¶é—´\"  # è‚¡å§æ•°æ®ä¸­çš„æ—¥æœŸåˆ—åç§°\n",
    "\n",
    "\n",
    "# ---------------------- 2. å·¥å…·å‡½æ•° ----------------------\n",
    "def load_stopwords(stop_path):\n",
    "    \"\"\"åŠ è½½åœç”¨è¯è¡¨\"\"\"\n",
    "    try:\n",
    "        with open(stop_path, 'r', encoding='utf-8') as f:\n",
    "            return set(f.read().splitlines())\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ åœç”¨è¯è¡¨åŠ è½½å¤±è´¥: {str(e)}ï¼Œå°†è·³è¿‡åœç”¨è¯è¿‡æ»¤\")\n",
    "        return set()\n",
    "\n",
    "\n",
    "def load_sentiment_dict(dict_path):\n",
    "    \"\"\"åŠ è½½æƒ…æ„Ÿè¯å…¸\"\"\"\n",
    "    try:\n",
    "        with open(dict_path, 'r', encoding='utf-8') as f:\n",
    "            raw = json.load(f)\n",
    "        return {w: (raw[w][\"sentiment\"], raw[w][\"weight\"]) for w in raw}\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"âŒ æƒ…æ„Ÿè¯å…¸åŠ è½½å¤±è´¥: {str(e)}\")\n",
    "\n",
    "\n",
    "def clean_text(text, stopwords, sent_dict):\n",
    "    \"\"\"æ¸…æ´—æ–‡æœ¬\"\"\"\n",
    "    text = str(text).strip()\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    # ä¿ç•™å…³é”®å­—ç¬¦\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\u4e00-\\u9fa5\\s%ï¿¥.\\+\\-!?]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    # è¿‡æ»¤åœç”¨è¯ï¼ˆä¿ç•™æƒ…æ„Ÿè¯å…¸ä¸­çš„è¯ï¼‰\n",
    "    if stopwords:\n",
    "        words = [w for w in text.split() if w not in stopwords or w in sent_dict]\n",
    "        return \" \".join(words) if words else text\n",
    "    return text\n",
    "\n",
    "\n",
    "# ---------------------- 3. æ•°æ®é›†å®šä¹‰ ----------------------\n",
    "class FastBarDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len):\n",
    "        self.texts = df['cleaned_text'].tolist()\n",
    "        self.titles = df['æ ‡é¢˜'].tolist()\n",
    "        self.times = df[FastConfig().BAR_DATE_COLUMN].tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            self.texts[idx],\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'title': self.titles[idx],\n",
    "            'time': self.times[idx]\n",
    "        }\n",
    "\n",
    "\n",
    "# ---------------------- 4. æ—¥æœŸå¤„ç†å·¥å…·å‡½æ•° ----------------------\n",
    "def parse_date_column(df, date_column, column_name):\n",
    "    \"\"\"è§£ææ—¥æœŸåˆ—ï¼Œæ”¯æŒå¤šç§æ ¼å¼\"\"\"\n",
    "    # å°è¯•å¤šç§æ—¥æœŸæ ¼å¼è§£æ\n",
    "    date_formats = [\n",
    "        '%Y-%m-%d', '%Y/%m/%d', '%Yå¹´%mæœˆ%dæ—¥',\n",
    "        '%d-%m-%Y', '%d/%m/%Y', '%m-%d-%Y', '%m/%d/%Y',\n",
    "        '%Y-%m-%d %H:%M:%S', '%Y/%m/%d %H:%M:%S'\n",
    "    ]\n",
    "    \n",
    "    for fmt in date_formats:\n",
    "        try:\n",
    "            parsed = pd.to_datetime(df[date_column], format=fmt, errors='coerce')\n",
    "            if parsed.notna().sum() > 0.5 * len(df):  # è‡³å°‘50%çš„æ•°æ®èƒ½è¢«è§£æ\n",
    "                return parsed.dt.date.astype(str)\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    # æœ€åå°è¯•è‡ªåŠ¨è§£æ\n",
    "    parsed = pd.to_datetime(df[date_column], errors='coerce')\n",
    "    valid_rate = parsed.notna().sum() / len(df)\n",
    "    \n",
    "    if valid_rate < 0.1:  # æœ‰æ•ˆè§£æç‡ä½äº10%\n",
    "        raise ValueError(f\"\"\"\n",
    "        âŒ æ—¥æœŸè§£æå¤±è´¥ï¼\n",
    "        æ— æ³•è§£æ{column_name}ä¸­çš„æ—¥æœŸæ ¼å¼ï¼Œè¯·æ£€æŸ¥ï¼š\n",
    "        1. åˆ—åæ˜¯å¦æ­£ç¡®ï¼šå½“å‰ä½¿ç”¨ '{date_column}'\n",
    "        2. æ—¥æœŸæ ¼å¼æ˜¯å¦ç¬¦åˆå¸¸è§æ ¼å¼ï¼ˆå¦‚2023-01-01ã€2023/01/01ï¼‰\n",
    "        3. æ•°æ®ä¸­æ˜¯å¦åŒ…å«éæ—¥æœŸå€¼\n",
    "        \"\"\")\n",
    "    \n",
    "    print(f\"âš ï¸ éƒ¨åˆ†æ—¥æœŸè§£æå¤±è´¥ï¼ˆæœ‰æ•ˆç‡ï¼š{valid_rate:.2%}ï¼‰ï¼Œå·²è‡ªåŠ¨å¤„ç†\")\n",
    "    return parsed.dt.date.astype(str)\n",
    "\n",
    "\n",
    "# ---------------------- 5. æ ¸å¿ƒæµç¨‹ ----------------------\n",
    "def fast_predict_and_merge():\n",
    "    config = FastConfig()\n",
    "    print(f\"âœ… å¼€å§‹ï¼šæƒ…æ„Ÿé¢„æµ‹ä¸æ•°æ®åˆå¹¶ï¼ˆè®¾å¤‡ï¼š{config.DEVICE.type}ï¼‰\")\n",
    "\n",
    "    # æ­¥éª¤1ï¼šåŠ è½½æ¨¡å‹\n",
    "    print(\"\\n1/3 åŠ è½½æ¨¡å‹...\")\n",
    "    try:\n",
    "        tokenizer = BertTokenizer.from_pretrained(\n",
    "            config.LOCAL_BERT, local_files_only=True, do_lower_case=False\n",
    "        )\n",
    "        model = BertForSequenceClassification.from_pretrained(\n",
    "            config.LOCAL_BERT, num_labels=3, local_files_only=True\n",
    "        )\n",
    "        model.load_state_dict(torch.load(config.MODEL_WEIGHT, map_location=config.DEVICE))\n",
    "        model.to(config.DEVICE).eval()\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}\\nè¯·æ£€æŸ¥æ¨¡å‹è·¯å¾„æ˜¯å¦æ­£ç¡®\")\n",
    "\n",
    "    # æ­¥éª¤2ï¼šå¤„ç†è‚¡å§æ•°æ®å¹¶é¢„æµ‹æƒ…æ„Ÿ\n",
    "    print(\"\\n2/3 å¤„ç†è‚¡å§æ•°æ®å¹¶é¢„æµ‹æƒ…æ„Ÿ...\")\n",
    "    try:\n",
    "        # åŠ è½½è‚¡å§æ•°æ®\n",
    "        if Path(config.BAR_DATA).suffix == '.xlsx':\n",
    "            bar_df = pd.read_excel(config.BAR_DATA)\n",
    "        else:\n",
    "            bar_df = pd.read_csv(config.BAR_DATA, encoding='utf-8-sig')\n",
    "        \n",
    "        # æ£€æŸ¥å¿…è¦å­—æ®µ\n",
    "        required_bar_cols = ['æ ‡é¢˜', config.BAR_DATE_COLUMN]\n",
    "        missing_bar_cols = [col for col in required_bar_cols if col not in bar_df.columns]\n",
    "        if missing_bar_cols:\n",
    "            raise ValueError(f\"âŒ è‚¡å§æ•°æ®ç¼ºå°‘å­—æ®µï¼š{missing_bar_cols}\")\n",
    "        \n",
    "        # æ•°æ®æ¸…æ´—\n",
    "        bar_df = bar_df[required_bar_cols].dropna().drop_duplicates('æ ‡é¢˜').reset_index(drop=True)\n",
    "        print(f\"  è‚¡å§æ•°æ®æ¸…æ´—åï¼š{len(bar_df)}æ¡æœ‰æ•ˆè®°å½•\")\n",
    "        \n",
    "        # æ–‡æœ¬é¢„å¤„ç†\n",
    "        stopwords = load_stopwords(config.STOPWORDS)\n",
    "        sent_dict = load_sentiment_dict(config.SENTIMENT_DICT)\n",
    "        bar_df['cleaned_text'] = bar_df['æ ‡é¢˜'].apply(lambda x: clean_text(x, stopwords, sent_dict))\n",
    "        bar_df = bar_df[bar_df['cleaned_text'].str.len() >= 3].reset_index(drop=True)\n",
    "        print(f\"  æ–‡æœ¬é¢„å¤„ç†åï¼š{len(bar_df)}æ¡æœ‰æ•ˆè®°å½•\")\n",
    "\n",
    "        # æ‰¹é‡é¢„æµ‹æƒ…æ„Ÿå¾—åˆ†\n",
    "        dataset = FastBarDataset(bar_df, tokenizer, config.MAX_LEN)\n",
    "        dataloader = DataLoader(dataset, batch_size=config.BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "        \n",
    "        all_scores = []\n",
    "        all_titles = []\n",
    "        all_times = []\n",
    "        with torch.no_grad():  # å…³é—­æ¢¯åº¦è®¡ç®—ï¼ŒåŠ é€Ÿ\n",
    "            for batch in tqdm(dataloader, desc=\"é¢„æµ‹è¿›åº¦\"):\n",
    "                input_ids = batch['input_ids'].to(config.DEVICE)\n",
    "                attention_mask = batch['attention_mask'].to(config.DEVICE)\n",
    "                logits = model(input_ids=input_ids, attention_mask=attention_mask).logits\n",
    "                probs = torch.softmax(logits, dim=1).cpu().numpy()\n",
    "                scores = probs[:, 2] - probs[:, 0]  # ç§¯ææ¦‚ç‡ - æ¶ˆææ¦‚ç‡\n",
    "                all_scores.extend(scores)\n",
    "                all_titles.extend(batch['title'])\n",
    "                all_times.extend(batch['time'])\n",
    "\n",
    "        # æ•´ç†æƒ…æ„Ÿå¾—åˆ†ç»“æœ\n",
    "        score_df = pd.DataFrame({\n",
    "            'æ ‡é¢˜': all_titles,\n",
    "            config.BAR_DATE_COLUMN: all_times,\n",
    "            'æƒ…æ„Ÿå¾—åˆ†': [round(s, 4) for s in all_scores]\n",
    "        })\n",
    "        bar_with_score = bar_df.merge(score_df, on=['æ ‡é¢˜', config.BAR_DATE_COLUMN], how='left')\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"âŒ è‚¡å§æ•°æ®å¤„ç†å¤±è´¥: {str(e)}\")\n",
    "\n",
    "    # æ­¥éª¤3ï¼šåˆå¹¶åˆ°è‚¡ç¥¨æ•°æ®\n",
    "    print(\"\\n3/3 åˆå¹¶åˆ°è‚¡ç¥¨æ•°æ®...\")\n",
    "    try:\n",
    "        # åŠ è½½è‚¡ç¥¨æ•°æ®\n",
    "        stock_df = pd.read_csv(config.STOCK_DATA, encoding='utf-8-sig')\n",
    "        \n",
    "        # æ˜¾ç¤ºè‚¡ç¥¨æ•°æ®åˆ—ä¿¡æ¯ï¼Œå¸®åŠ©è°ƒè¯•\n",
    "        print(f\"  è‚¡ç¥¨æ•°æ®åŒ…å«åˆ—ï¼š{stock_df.columns.tolist()}\")\n",
    "        \n",
    "        # æ£€æŸ¥è‚¡ç¥¨æ—¥æœŸåˆ—æ˜¯å¦å­˜åœ¨\n",
    "        if config.STOCK_DATE_COLUMN not in stock_df.columns:\n",
    "            raise ValueError(f\"âŒ è‚¡ç¥¨æ•°æ®ä¸­æœªæ‰¾åˆ°æ—¥æœŸåˆ— '{config.STOCK_DATE_COLUMN}'\\nè¯·ä¿®æ”¹STOCK_DATE_COLUMNä¸ºå®é™…åˆ—å\")\n",
    "        \n",
    "        # è§£ææ—¥æœŸï¼ˆæ ¸å¿ƒä¿®å¤éƒ¨åˆ†ï¼‰\n",
    "        print(f\"  è§£æè‚¡å§æ—¥æœŸåˆ—ï¼š{config.BAR_DATE_COLUMN}\")\n",
    "        bar_with_score['æ—¥æœŸ'] = parse_date_column(bar_with_score, config.BAR_DATE_COLUMN, \"è‚¡å§æ•°æ®\")\n",
    "        \n",
    "        print(f\"  è§£æè‚¡ç¥¨æ—¥æœŸåˆ—ï¼š{config.STOCK_DATE_COLUMN}\")\n",
    "        stock_df['æ—¥æœŸ'] = parse_date_column(stock_df, config.STOCK_DATE_COLUMN, \"è‚¡ç¥¨æ•°æ®\")\n",
    "        \n",
    "        # è¿‡æ»¤æ— æ•ˆæ—¥æœŸ\n",
    "        bar_with_score = bar_with_score[bar_with_score['æ—¥æœŸ'] != 'NaT'].reset_index(drop=True)\n",
    "        stock_df = stock_df[stock_df['æ—¥æœŸ'] != 'NaT'].reset_index(drop=True)\n",
    "\n",
    "        # è®¡ç®—æ¯æ—¥å¹³å‡æƒ…æ„Ÿå¾—åˆ†\n",
    "        daily_score = bar_with_score.groupby('æ—¥æœŸ')['æƒ…æ„Ÿå¾—åˆ†'].agg(['mean', 'count']).reset_index()\n",
    "        daily_score.columns = ['æ—¥æœŸ', 'æ¯æ—¥å¹³å‡æƒ…æ„Ÿå¾—åˆ†', 'å½“æ—¥è‚¡å§è¯„è®ºæ•°']\n",
    "        print(f\"  è®¡ç®—å®Œæˆï¼š{len(daily_score)}ä¸ªæ—¥æœŸçš„æƒ…æ„Ÿå¾—åˆ†\")\n",
    "\n",
    "        # åˆå¹¶æ•°æ®\n",
    "        merged_df = stock_df.merge(\n",
    "            daily_score, on='æ—¥æœŸ', how='left'\n",
    "        ).fillna({'æ¯æ—¥å¹³å‡æƒ…æ„Ÿå¾—åˆ†': 0.0, 'å½“æ—¥è‚¡å§è¯„è®ºæ•°': 0})\n",
    "\n",
    "        # ä¿å­˜ç»“æœ\n",
    "        merged_df.to_csv(config.OUTPUT, index=False, encoding='utf-8-sig')\n",
    "        print(f\"\\nğŸ‰ å…¨éƒ¨å®Œæˆï¼ç»“æœå·²ä¿å­˜è‡³ï¼š{config.OUTPUT}\")\n",
    "        \n",
    "        # é¢„è§ˆç»“æœ\n",
    "        print(\"\\nç»“æœé¢„è§ˆï¼ˆå‰3è¡Œï¼‰ï¼š\")\n",
    "        preview_cols = ['æ—¥æœŸ', 'æ¯æ—¥å¹³å‡æƒ…æ„Ÿå¾—åˆ†', 'å½“æ—¥è‚¡å§è¯„è®ºæ•°', 'open', 'close']\n",
    "        preview_cols = [c for c in preview_cols if c in merged_df.columns]\n",
    "        print(merged_df[preview_cols].head(3))\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"âŒ è‚¡ç¥¨æ•°æ®åˆå¹¶å¤±è´¥: {str(e)}\")\n",
    "\n",
    "\n",
    "# ---------------------- è¿è¡Œå…¥å£ ----------------------\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        fast_predict_and_merge()\n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ æ‰§è¡Œç»ˆæ­¢ï¼š{str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa98ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Bidirectional\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# åŠ è½½æ•°æ®\n",
    "df = pd.read_csv('ç‰ç±³_å¸¦æƒ…æ„Ÿå¾—åˆ†.csv')\n",
    "\n",
    "# å‡è®¾ä½¿ç”¨æ¯æ—¥å¹³å‡æƒ…æ„Ÿå¾—åˆ†å’Œå½“æ—¥è‚¡å§è¯„è®ºæ•°æ¥é¢„æµ‹æ”¶ç›˜ä»·\n",
    "features = ['æ¯æ—¥å¹³å‡æƒ…æ„Ÿå¾—åˆ†', 'å½“æ—¥è‚¡å§è¯„è®ºæ•°', 'close']\n",
    "data = df[features].values\n",
    "\n",
    "# å½’ä¸€åŒ–æ•°æ®\n",
    "sc = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_data = sc.fit_transform(data)\n",
    "\n",
    "# åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†\n",
    "train_size = int(len(scaled_data) * 0.8)\n",
    "train_data = scaled_data[:train_size, :]\n",
    "test_data = scaled_data[train_size:, :]\n",
    "\n",
    "# å‡†å¤‡è®­ç»ƒæ•°æ®\n",
    "def create_dataset(dataset, time_step=1):\n",
    "    X, y = [], []\n",
    "    for i in range(len(dataset) - time_step - 1):\n",
    "        X.append(dataset[i:(i + time_step), :])\n",
    "        y.append(dataset[i + time_step, 2])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "time_step = 10\n",
    "X_train, y_train = create_dataset(train_data, time_step)\n",
    "X_test, y_test = create_dataset(test_data, time_step)\n",
    "\n",
    "# å®šä¹‰æ¨¡å‹ç±»å‹ä¸ºå¤šå±‚ LSTM\n",
    "model_type = 1\n",
    "n_epochs = 20\n",
    "\n",
    "# å»ºæ„ LSTM æ¨¡å‹\n",
    "if model_type == 1:\n",
    "    # å•å±‚ LSTM\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=50, activation='relu',\n",
    "                   input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "    model.add(Dense(units=1))\n",
    "if model_type == 2:\n",
    "    # å¤šå±‚ LSTM\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=50, activation='relu', return_sequences=True,\n",
    "                   input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "    model.add(LSTM(units=50, activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "if model_type == 3:\n",
    "    # åŒå‘ LSTM\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSTM(50, activation='relu'),\n",
    "                            input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "    model.add(Dense(1))\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# è®­ç»ƒæ¨¡å‹\n",
    "history = model.fit(X_train, y_train, batch_size=64, epochs=n_epochs,\n",
    "                    validation_data=(X_test, y_test), validation_freq=1)\n",
    "\n",
    "# é¢„æµ‹\n",
    "train_predict = model.predict(X_train)\n",
    "test_predict = model.predict(X_test)\n",
    "\n",
    "# åå½’ä¸€åŒ–é¢„æµ‹ç»“æœ\n",
    "# æ„å»ºä¸åŸå§‹æ•°æ®ç›¸åŒç»“æ„çš„æ•°ç»„ï¼Œç”¨äºåå½’ä¸€åŒ–\n",
    "train_predict_extended = np.zeros((len(train_predict), data.shape[1]))\n",
    "train_predict_extended[:, 2] = train_predict[:, 0]\n",
    "train_predict = sc.inverse_transform(train_predict_extended)[:, 2]\n",
    "\n",
    "test_predict_extended = np.zeros((len(test_predict), data.shape[1]))\n",
    "test_predict_extended[:, 2] = test_predict[:, 0]\n",
    "test_predict = sc.inverse_transform(test_predict_extended)[:, 2]\n",
    "\n",
    "# åå½’ä¸€åŒ–çœŸå®å€¼\n",
    "y_train_extended = np.zeros((len(y_train), data.shape[1]))\n",
    "y_train_extended[:, 2] = y_train\n",
    "y_train = sc.inverse_transform(y_train_extended)[:, 2]\n",
    "\n",
    "y_test_extended = np.zeros((len(y_test), data.shape[1]))\n",
    "y_test_extended[:, 2] = y_test\n",
    "y_test = sc.inverse_transform(y_test_extended)[:, 2]\n",
    "\n",
    "# è®¡ç®—è¯„ä¼°æŒ‡æ ‡\n",
    "mse_train = mean_squared_error(y_train, train_predict)\n",
    "mse_test = mean_squared_error(y_test, test_predict)\n",
    "rmse_train = np.sqrt(mse_train)\n",
    "rmse_test = np.sqrt(mse_test)\n",
    "mae_train = mean_absolute_error(y_train, train_predict)\n",
    "mae_test = mean_absolute_error(y_test, test_predict)\n",
    "\n",
    "print('è®­ç»ƒé›† MSE:', mse_train)\n",
    "print('æµ‹è¯•é›† MSE:', mse_test)\n",
    "print('è®­ç»ƒé›† RMSE:', rmse_train)\n",
    "print('æµ‹è¯•é›† RMSE:', rmse_test)\n",
    "print('è®­ç»ƒé›† MAE:', mae_train)\n",
    "print('æµ‹è¯•é›† MAE:', mae_test)\n",
    "\n",
    "# ç»˜åˆ¶è®­ç»ƒå’ŒéªŒè¯æŸå¤±å›¾\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training and Validation Loss by LSTM')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a08cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Bidirectional\n",
    "import matplotlib.pyplot as plt\n",
    "# æ–°å¢å¯¼å…¥MAPEè®¡ç®—å‡½æ•°\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
    "\n",
    "# åŠ è½½æ•°æ®\n",
    "df = pd.read_csv('ç‰ç±³_å¸¦æƒ…æ„Ÿå¾—åˆ†.csv')\n",
    "\n",
    "# å‡è®¾ä½¿ç”¨æ¯æ—¥å¹³å‡æƒ…æ„Ÿå¾—åˆ†å’Œå½“æ—¥è‚¡å§è¯„è®ºæ•°æ¥é¢„æµ‹æ”¶ç›˜ä»·\n",
    "features = ['æ¯æ—¥å¹³å‡æƒ…æ„Ÿå¾—åˆ†', 'å½“æ—¥è‚¡å§è¯„è®ºæ•°', 'close']\n",
    "data = df[features].values\n",
    "\n",
    "# å½’ä¸€åŒ–æ•°æ®\n",
    "sc = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_data = sc.fit_transform(data)\n",
    "\n",
    "# åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†\n",
    "train_size = int(len(scaled_data) * 0.8)\n",
    "train_data = scaled_data[:train_size, :]\n",
    "test_data = scaled_data[train_size:, :]\n",
    "\n",
    "# å‡†å¤‡è®­ç»ƒæ•°æ®\n",
    "def create_dataset(dataset, time_step=1):\n",
    "    X, y = [], []\n",
    "    for i in range(len(dataset) - time_step - 1):\n",
    "        X.append(dataset[i:(i + time_step), :])\n",
    "        y.append(dataset[i + time_step, 2])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "time_step = 10\n",
    "X_train, y_train = create_dataset(train_data, time_step)\n",
    "X_test, y_test = create_dataset(test_data, time_step)\n",
    "\n",
    "# å®šä¹‰æ¨¡å‹ç±»å‹ä¸ºå¤šå±‚ LSTM\n",
    "model_type = 2\n",
    "n_epochs = 10\n",
    "\n",
    "# å»ºæ„ LSTM æ¨¡å‹\n",
    "if model_type == 1:\n",
    "    # å•å±‚ LSTM\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=50, activation='relu',\n",
    "                   input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "    model.add(Dense(units=1))\n",
    "if model_type == 2:\n",
    "    # å¤šå±‚ LSTM\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=50, activation='relu', return_sequences=True,\n",
    "                   input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "    model.add(LSTM(units=50, activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "if model_type == 3:\n",
    "    # åŒå‘ LSTM\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSTM(50, activation='relu'),\n",
    "                            input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "    model.add(Dense(1))\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# è®­ç»ƒæ¨¡å‹\n",
    "history = model.fit(X_train, y_train, batch_size=64, epochs=n_epochs,\n",
    "                    validation_data=(X_test, y_test), validation_freq=1)\n",
    "\n",
    "# é¢„æµ‹\n",
    "train_predict = model.predict(X_train)\n",
    "test_predict = model.predict(X_test)\n",
    "\n",
    "# åå½’ä¸€åŒ–é¢„æµ‹ç»“æœ\n",
    "# æ„å»ºä¸åŸå§‹æ•°æ®ç›¸åŒç»“æ„çš„æ•°ç»„ï¼Œç”¨äºåå½’ä¸€åŒ–\n",
    "train_predict_extended = np.zeros((len(train_predict), data.shape[1]))\n",
    "train_predict_extended[:, 2] = train_predict[:, 0]\n",
    "train_predict = sc.inverse_transform(train_predict_extended)[:, 2]\n",
    "\n",
    "test_predict_extended = np.zeros((len(test_predict), data.shape[1]))\n",
    "test_predict_extended[:, 2] = test_predict[:, 0]\n",
    "test_predict = sc.inverse_transform(test_predict_extended)[:, 2]\n",
    "\n",
    "# åå½’ä¸€åŒ–çœŸå®å€¼\n",
    "y_train_extended = np.zeros((len(y_train), data.shape[1]))\n",
    "y_train_extended[:, 2] = y_train\n",
    "y_train = sc.inverse_transform(y_train_extended)[:, 2]\n",
    "\n",
    "y_test_extended = np.zeros((len(y_test), data.shape[1]))\n",
    "y_test_extended[:, 2] = y_test\n",
    "y_test = sc.inverse_transform(y_test_extended)[:, 2]\n",
    "\n",
    "# è®¡ç®—è¯„ä¼°æŒ‡æ ‡ï¼ˆæ–°å¢MAPEè®¡ç®—ï¼‰\n",
    "mse_train = mean_squared_error(y_train, train_predict)\n",
    "mse_test = mean_squared_error(y_test, test_predict)\n",
    "rmse_train = np.sqrt(mse_train)\n",
    "rmse_test = np.sqrt(mse_test)\n",
    "mae_train = mean_absolute_error(y_train, train_predict)\n",
    "mae_test = mean_absolute_error(y_test, test_predict)\n",
    "# æ–°å¢MAPEè®¡ç®—ï¼ˆä¹˜ä»¥100è½¬ä¸ºç™¾åˆ†æ¯”å½¢å¼ï¼Œæ›´ç›´è§‚ï¼‰\n",
    "mape_train = mean_absolute_percentage_error(y_train, train_predict) * 100\n",
    "mape_test = mean_absolute_percentage_error(y_test, test_predict) * 100\n",
    "\n",
    "# æ‰“å°æŒ‡æ ‡ï¼ˆæ–°å¢MAPEè¾“å‡ºï¼‰\n",
    "print('è®­ç»ƒé›† MSE:', mse_train)\n",
    "print('æµ‹è¯•é›† MSE:', mse_test)\n",
    "print('è®­ç»ƒé›† RMSE:', rmse_train)\n",
    "print('æµ‹è¯•é›† RMSE:', rmse_test)\n",
    "print('è®­ç»ƒé›† MAE:', mae_train)\n",
    "print('æµ‹è¯•é›† MAE:', mae_test)\n",
    "print('è®­ç»ƒé›† MAPE (%):', mape_train)  # æ–°å¢MAPEæ‰“å°\n",
    "print('æµ‹è¯•é›† MAPE (%):', mape_test)  # æ–°å¢MAPEæ‰“å°\n",
    "\n",
    "# ç»˜åˆ¶è®­ç»ƒå’ŒéªŒè¯æŸå¤±å›¾\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training and Validation Loss by LSTM')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
