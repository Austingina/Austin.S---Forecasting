{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1cff948",
   "metadata": {},
   "source": [
    "# Get Financial Research Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473ac3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import atrader as at\n",
    "import numpy as np\n",
    "from eastmoney_scraper_many1 import EastmoneyReportScraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c8ee05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eastmoney_data(start_date=\"2025-04-09 00:00:00\", end_date=\"2025-04-10 00:00:00\", output_file=\"eastmoney_report_data.xlsx\", desired_count=1):\n",
    "    # Initialize the crawler (custom cookies can be passed in)\n",
    "    scraper = EastmoneyReportScraper()\n",
    "    scraper.scrape( start_date, end_date, output_file, desired_count=desired_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf7ac5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    keyword =[['期货'],['股票'],['股指']] #keysords futures, stock, index\n",
    "    start_date = '2025-05-24'\n",
    "    end_date = '2025-05-31'\n",
    "    output_file = 'eastmoney_report_data.xlsx'\n",
    "    desired_count = 100\n",
    "\n",
    "    get_eastmoney_data(start_date, end_date, output_file, desired_count=desired_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03f5d1b",
   "metadata": {},
   "source": [
    "# Generate sentiment dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991e6300",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "import jieba\n",
    "import numpy as np\n",
    "from snownlp import SnowNLP\n",
    "from gensim.models import KeyedVectors\n",
    "import PyPDF2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9dd678f",
   "metadata": {},
   "source": [
    "Read the research report from the folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304433ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_reports_from_folder(folder_path):\n",
    "    reports = []\n",
    "    # Use the os.walk function to traverse the specified folder and its subfolders\n",
    "    # The os.walk function returns a tuple (root, dirs, files)\n",
    "    # root represents the path of the current directory being traversed\n",
    "    # dirs is a list of subdirectory names in the current directory\n",
    "    # files is a list containing the names of the files in the current directory\n",
    "    \n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            if file.endswith('.pdf'):\n",
    "                file_path = os.path.join(root, file)  # Combine multiple path components into a valid path string\n",
    "                try:\n",
    "                    with open(file_path, 'rb') as f:  # Note the use of binary mode 'rb'\n",
    "                        reader = PyPDF2.PdfReader(f)\n",
    "                        content = \"\"\n",
    "                        for page in reader.pages:\n",
    "                            content += page.extract_text()\n",
    "                        reports.append({\n",
    "                            'filename': file,\n",
    "                            'content': content\n",
    "                        })\n",
    "                except Exception as e:\n",
    "                    print(f\"Error reading file {file_path}: {e}\")\n",
    "    return reports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ee3594",
   "metadata": {},
   "source": [
    "Processing of clauses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfce448a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocess text and split it into sentences\n",
    "    1. Remove special characters (retain Chinese, English, and numbers)\n",
    "    2. Remove extra spaces\n",
    "    3. Chinese sentence segmentation (based on common punctuation)\n",
    "    \"\"\"\n",
    "    # Remove special characters (retain Chinese, English, numbers, and basic punctuation)\n",
    "    text = re.sub(r'[^\\u4e00-\\u9fa5a-zA-Z0-9,.!?;:\\s]', ' ', text)\n",
    "    \n",
    "    # Remove extra spaces, \\s+ matches one or more consecutive whitespace characters\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()    \n",
    "    \n",
    "    # Chinese sentence segmentation (split by common punctuation)\n",
    "    sentences = re.split(r'([.!?;:])', text)\n",
    "    \n",
    "    # Recombine sentences (attach punctuation back to the previous sentence)\n",
    "    processed_sentences = []\n",
    "    for i in range(0, len(sentences)-1, 2):\n",
    "        sentence = sentences[i] + (sentences[i+1] if i+1 < len(sentences) else '')\n",
    "        if sentence.strip():  # Remove empty sentences\n",
    "            processed_sentences.append(sentence.strip())\n",
    "    \n",
    "    # Handle the last element if there's an odd number of elements\n",
    "    if len(sentences) % 2 != 0 and sentences[-1].strip():\n",
    "        processed_sentences.append(sentences[-1].strip())\n",
    "    \n",
    "    return processed_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a8ea23",
   "metadata": {},
   "source": [
    "Initially construct financial sentiment dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e47b143",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_chinese_word(word):\n",
    "    \"\"\"Check if a word consists purely of Chinese characters\"\"\"\n",
    "    for char in word:\n",
    "        if not '\\u4e00' <= char <= '\\u9fff':\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def build_sentiment_dictionary(positive_texts, negative_texts, min_freq=3):\n",
    "    \"\"\"\n",
    "    Build a sentiment dictionary (only retaining pure Chinese words)\n",
    "    positive_texts: List of positive texts\n",
    "    negative_texts: List of negative texts\n",
    "    min_freq: Minimum frequency threshold\n",
    "    \"\"\"\n",
    "    # Create frequency dictionaries\n",
    "    pos_word_freq = defaultdict(int)\n",
    "    neg_word_freq = defaultdict(int)\n",
    "    \n",
    "    # Analyze positive texts\n",
    "    for text in positive_texts:\n",
    "        words = jieba.lcut(text)\n",
    "        for word in words:\n",
    "            # Only retain pure Chinese words longer than 1 character\n",
    "            if len(word) > 1 and is_chinese_word(word):\n",
    "                pos_word_freq[word] += 1\n",
    "    \n",
    "    # Analyze negative texts\n",
    "    for text in negative_texts:\n",
    "        words = jieba.lcut(text)\n",
    "        for word in words:\n",
    "            # Only retain pure Chinese words longer than 1 character\n",
    "            if len(word) > 1 and is_chinese_word(word):\n",
    "                neg_word_freq[word] += 1\n",
    "    \n",
    "    # Build sentiment dictionary\n",
    "    sentiment_dict = {}\n",
    "    \n",
    "    # Add positive words\n",
    "    for word, freq in pos_word_freq.items():\n",
    "        if freq >= min_freq and word not in sentiment_dict:\n",
    "            sentiment_dict[word] = {\n",
    "                'sentiment': 'positive',\n",
    "                'frequency': freq,\n",
    "                'source': 'auto_generated'\n",
    "            }\n",
    "    \n",
    "    # Add negative words\n",
    "    for word, freq in neg_word_freq.items():\n",
    "        if freq >= min_freq:\n",
    "            if word in sentiment_dict:\n",
    "                # If a word appears in both positive and negative texts, compare frequencies\n",
    "                if freq > sentiment_dict[word]['frequency']:\n",
    "                    sentiment_dict[word] = {\n",
    "                        'sentiment': 'negative',\n",
    "                        'frequency': freq,\n",
    "                        'source': 'auto_generated'\n",
    "                    }\n",
    "            else:\n",
    "                sentiment_dict[word] = {\n",
    "                    'sentiment': 'negative',\n",
    "                    'frequency': freq,\n",
    "                    'source': 'auto_generated'\n",
    "                }\n",
    "    \n",
    "    return sentiment_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb78cd53",
   "metadata": {},
   "source": [
    "Preliminary labeling of emotion labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a032a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    output_file = '2025-05-24_2025-05-31'\n",
    "    \n",
    "    # Create save directory\n",
    "    save_dir = os.path.join(output_file, 'sentiment_results')\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Initialize storage\n",
    "    positive_texts = []\n",
    "    negative_texts = []\n",
    "    \n",
    "    # Read research reports from folder\n",
    "    reports = read_reports_from_folder(output_file)\n",
    "    processed_reports = []\n",
    "    \n",
    "    for report in reports:\n",
    "        # Preprocess each report's content into sentences\n",
    "        processed_content = preprocess_text(report['content'])\n",
    "        processed_reports.append({\n",
    "            'filename': report['filename'],\n",
    "            'content': processed_content  # This is a list of sentences\n",
    "        })\n",
    "\n",
    "    # Perform sentiment analysis and classify texts\n",
    "    for report in processed_reports:\n",
    "        print(f\"\\nAnalyzing file: {report['filename']}\")\n",
    "        \n",
    "        # Create separate positive and negative result files for each report\n",
    "        report_name = os.path.splitext(report['filename'])[0]\n",
    "        pos_file = os.path.join(save_dir, f\"{report_name}_positive.txt\")\n",
    "        neg_file = os.path.join(save_dir, f\"{report_name}_negative.txt\")\n",
    "        \n",
    "        # Calculate sentiment scores using SnowNLP for each sentence in the report\n",
    "        # Classify sentences as positive or negative and save to corresponding files\n",
    "        with open(pos_file, 'w', encoding='utf-8') as f_pos, \\\n",
    "             open(neg_file, 'w', encoding='utf-8') as f_neg:\n",
    "            \n",
    "            for i, sentence in enumerate(report['content'], 1):\n",
    "                if len(sentence) < 5:\n",
    "                    continue\n",
    "                    \n",
    "                try:\n",
    "                    s = SnowNLP(sentence)\n",
    "                    sentiment_score = s.sentiments\n",
    "                    \n",
    "                    if sentiment_score >= 0.7:\n",
    "                        f_pos.write(f\"{sentiment_score:.4f}\\t{sentence}\\n\")\n",
    "                        positive_texts.append(sentence)\n",
    "                    elif sentiment_score <= 0.3:\n",
    "                        f_neg.write(f\"{sentiment_score:.4f}\\t{sentence}\\n\")\n",
    "                        negative_texts.append(sentence)\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"Error analyzing sentence {i}: {str(e)}\")\n",
    "                    continue\n",
    "    \n",
    "    # Build sentiment dictionary based on word frequency in positive and negative texts\n",
    "    sentiment_dict = build_sentiment_dictionary(positive_texts, negative_texts)\n",
    "    \n",
    "    # Save sentiment dictionary\n",
    "    dict_file = os.path.join(save_dir, 'financial_sentiment_dict.json')\n",
    "    with open(dict_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(sentiment_dict, f, ensure_ascii=False, indent=2)    # json.dump() converts a Python object (usually a dictionary or list) into a JSON-formatted string. Setting ensure_ascii=False ensures non-ASCII characters (like Chinese) are output correctly.\n",
    "    \n",
    "    print(\"\\nSentiment dictionary construction completed:\")\n",
    "    print(f\"- Number of positive texts: {len(positive_texts)}\")\n",
    "    print(f\"- Number of negative texts: {len(negative_texts)}\")\n",
    "    print(f\"- Size of sentiment dictionary: {len(sentiment_dict)}\")\n",
    "    print(f\"Results saved to: {save_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e450c89",
   "metadata": {},
   "source": [
    "# Augmented sentiment dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a441d3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bz2\n",
    "\n",
    "# Path to the compressed file\n",
    "compressed_file = 'sgns.financial.word.bz2'\n",
    "# Path to the decompressed file\n",
    "decompressed_file = 'sgns.financial.word'\n",
    "\n",
    "with bz2.open(compressed_file, 'rb') as f_in:       # 'rb' indicates opening in read-only binary mode\n",
    "    with open(decompressed_file, 'wb') as f_out:   # 'wb' indicates opening in write binary mode\n",
    "        f_out.write(f_in.read())        # Read all content from the compressed file and write it to the decompressed file\n",
    "\n",
    "print(\"File decompression completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6ae461",
   "metadata": {},
   "source": [
    "# Dictionary construction and extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d4d9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_sentiment_dictionary(sentiment_dict, reports, word2vec_model, min_similarity=0.5):\n",
    "    \"\"\"\n",
    "    Expand the sentiment dictionary by analyzing sentiment words not covered in the initial dictionary\n",
    "    using the WORD2VEC word vector model to identify similar terms in research reports.\n",
    "    \n",
    "    sentiment_dict: Initial sentiment dictionary\n",
    "    reports: List of processed research reports\n",
    "    word2vec_model: Word2Vec word vector model\n",
    "    min_similarity: Minimum similarity threshold\n",
    "    return: Expanded sentiment dictionary\n",
    "    \"\"\"\n",
    "    new_sentiment_dict = sentiment_dict.copy()\n",
    "    for report in reports:\n",
    "        for sentence in report['content']:\n",
    "            words = jieba.lcut(sentence)\n",
    "            for word in words:\n",
    "                if len(word) > 1 and is_chinese_word(word) and word not in new_sentiment_dict:\n",
    "                    similar_words = []\n",
    "                    try:\n",
    "                        # Find words similar to known sentiment terms\n",
    "                        for known_word in sentiment_dict.keys():\n",
    "                            if word2vec_model.has_index_for(known_word) and word2vec_model.has_index_for(word):  # has_index_for checks word vector existence\n",
    "                                similarity = word2vec_model.similarity(known_word, word)\n",
    "                                if similarity >= min_similarity:\n",
    "                                    similar_words.append((known_word, similarity))\n",
    "                    except KeyError:\n",
    "                        continue\n",
    "\n",
    "                    if similar_words:\n",
    "                        # Determine sentiment of new word based on similar terms\n",
    "                        pos_count = sum([1 for known_word, _ in similar_words if sentiment_dict[known_word]['sentiment'] == 'positive'])\n",
    "                        neg_count = sum([1 for known_word, _ in similar_words if sentiment_dict[known_word]['sentiment'] == 'negative'])\n",
    "                        if pos_count > neg_count:\n",
    "                            new_sentiment_dict[word] = {\n",
    "                               'sentiment': 'positive',\n",
    "                                'frequency': 1,\n",
    "                               'source': 'expanded_by_word2vec'\n",
    "                            }\n",
    "                        elif neg_count > pos_count:\n",
    "                            new_sentiment_dict[word] = {\n",
    "                               'sentiment': 'negative',\n",
    "                                'frequency': 1,\n",
    "                               'source': 'expanded_by_word2vec'\n",
    "                            }\n",
    "                        else:\n",
    "                            new_sentiment_dict[word] = {\n",
    "                               'sentiment': 'neutral',\n",
    "                                'frequency': 1,\n",
    "                               'source': 'expanded_by_word2vec'\n",
    "                            }\n",
    "                    else:\n",
    "                        new_sentiment_dict[word] = {\n",
    "                           'sentiment': 'neutral',\n",
    "                            'frequency': 1,\n",
    "                           'source': 'new_word'\n",
    "                        }\n",
    "    return new_sentiment_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340cade5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_sentiment_weights(sentiment_dict, reports):\n",
    "    \"\"\"\n",
    "    Adjust sentiment weights of words in the dictionary based on their contextual sentiment performance\n",
    "    \n",
    "    sentiment_dict: Sentiment dictionary\n",
    "    reports: List of processed research reports\n",
    "    return: Sentiment dictionary with adjusted weights\n",
    "    \"\"\"\n",
    "    for report in reports:\n",
    "        for sentence in report['content']:\n",
    "            words = jieba.lcut(sentence)\n",
    "            s = SnowNLP(sentence)\n",
    "            sentiment_score = s.sentiments\n",
    "            for word in words:\n",
    "                if word in sentiment_dict:\n",
    "                    if sentiment_score >= 0.7:\n",
    "                        if sentiment_dict[word]['sentiment'] == 'positive':\n",
    "                            sentiment_dict[word]['frequency'] += 1\n",
    "                    elif sentiment_score <= 0.3:\n",
    "                        if sentiment_dict[word]['sentiment'] == 'negative':\n",
    "                            sentiment_dict[word]['frequency'] += 1\n",
    "    return sentiment_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968ba937",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    # Load Word2Vec word vector model\n",
    "    word2vec_model = KeyedVectors.load_word2vec_format('sgns.financial.word', binary=False)\n",
    "\n",
    "    # Expand sentiment dictionary (if uncovered words have >80% similarity with existing sentiment words in Word2Vec vectors,\n",
    "    # they are counted as similar words. Determine sentiment based on similar words - if more positive than negative,\n",
    "    # classify as positive and add to the dictionary)\n",
    "    sentiment_dict = expand_sentiment_dictionary(sentiment_dict, processed_reports, word2vec_model)\n",
    "\n",
    "    # Adjust sentiment weights (use SnowNLP to determine sentiment scores: words in sentences with score ≥0.7 are considered\n",
    "    # positive, those in sentences with score ≤0.3 are considered negative; increase their frequency accordingly)\n",
    "    sentiment_dict = adjust_sentiment_weights(sentiment_dict, processed_reports)\n",
    "\n",
    "    # Save sentiment dictionary\n",
    "    dict_file = os.path.join(save_dir, 'financial_sentiment_dict_0.5.json')\n",
    "    with open(dict_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(sentiment_dict, f, ensure_ascii=False, indent=2)   # json.dump() converts Python objects (usually dictionaries or lists) to JSON-formatted strings\n",
    "\n",
    "    print(\"\\nSentiment dictionary construction completed:\")\n",
    "    print(f\"- Number of positive texts: {len(positive_texts)}\")\n",
    "    print(f\"- Number of negative texts: {len(negative_texts)}\")\n",
    "    print(f\"- Size of sentiment dictionary: {len(sentiment_dict)}\")\n",
    "    print(f\"Results saved to: {save_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7a7797",
   "metadata": {},
   "source": [
    "# Sentiment Analysis and Prediction Pipeline for Financial Market Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d886610b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------- 0. Configurable Keyword ----------------------\n",
    "KEYWORD = \"Corn\"  # e.g：\"China_Unicom\"、\"AMZN\"、\"Corn\"、\"CSI100\" \n",
    "\n",
    "\n",
    "# ---------------------- 1. Import Dependent Libraries ----------------------\n",
    "import pandas as pd\n",
    "import jieba\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from pathlib import Path\n",
    "import torch.nn as nn \n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "\n",
    "# ---------------------- 2. Hyperparameter Configuration ----------------------\n",
    "# Path configuration (modify according to actual paths)\n",
    "DATA_PATH = f\"data/{KEYWORD}_stock_bar_data_2020-2024.xlsx\"  # AMZN_stock_bar_data_2017-2023.xlsx\n",
    "DICT_PATH = \"financial_sentiment_dict_100.json\"           # Sentiment dictionary path\n",
    "STOPWORDS_PATH = \"data/stopwords.txt\"   # Stopwords list path\n",
    "OUTPUT_PATH = f\"data/{KEYWORD}_data_with_sentiment_scores.csv\"    # Final output path\n",
    "MODEL_SAVE_PATH = f\"data/{KEYWORD}_best_sentiment_model.pth\"   # Model save path\n",
    "\n",
    "# Model parameters\n",
    "EMBED_DIM = 100  # Word embedding dimension\n",
    "HIDDEN_DIM = 256  # LSTM hidden layer dimension\n",
    "BATCH_SIZE = 32  # Batch size\n",
    "EPOCHS = 30  # Training epochs\n",
    "MAX_LEN = 20  # Maximum text length (titles are usually short)\n",
    "EARLY_STOPPING_PATIENCE = 3  # Early stopping patience\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------- 3. Tool Function Definitions ----------------------\n",
    "def load_stopwords(path):\n",
    "    \"\"\"Load stopwords list\"\"\"\n",
    "    \n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        return set(f.read().splitlines())\n",
    "\n",
    "def preprocess_text(text, stopwords):\n",
    "    \"\"\"Text preprocessing: word segmentation + stopword filtering\"\"\"\n",
    "    text = str(text).strip()\n",
    "    if not text:\n",
    "        return []\n",
    "    words = jieba.lcut(text)\n",
    "    # Retain single-character sentiment words (e.g., \"rise\", \"fall\"), only filter meaningless stopwords\n",
    "    return [word for word in words if word not in stopwords or len(word) >= 1]\n",
    "\n",
    "def load_sentiment_dict(path):\n",
    "    \"\"\"Load sentiment dictionary and convert to {word: (sentiment tendency, frequency)}\"\"\"\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        raw_dict = json.load(f)\n",
    "    return {word: (info['sentiment'], info['frequency']) for word, info in raw_dict.items()}\n",
    "\n",
    "def generate_pseudo_labels(tokens, sent_dict):\n",
    "    \"\"\"Generate pseudo-labels based on the number of sentiment words (positive/negative/neutral)\"\"\"\n",
    "    pos_count = sum(1 for word in tokens if sent_dict.get(word, (None,))[0] == 'positive')\n",
    "    neg_count = sum(1 for word in tokens if sent_dict.get(word, (None,))[0] == 'negative')\n",
    "    if pos_count > neg_count:\n",
    "        return 2  # Positive\n",
    "    elif neg_count > pos_count:\n",
    "        return 1  # Negative\n",
    "    else:\n",
    "        return 0  # Neutral\n",
    "\n",
    "\n",
    "# ---------------------- 4. Data Loading and Preprocessing ----------------------\n",
    "# 1. Load base data\n",
    "print(\"===== Data Loading and Cleaning =====\")\n",
    "try:\n",
    "    if Path(DATA_PATH).suffix == '.xlsx':\n",
    "        df = pd.read_excel(DATA_PATH)\n",
    "    else:\n",
    "        df = pd.read_csv(DATA_PATH)\n",
    "except FileNotFoundError:\n",
    "    raise ValueError(f\"File not found: {DATA_PATH}, please check the path\")\n",
    "\n",
    "# 2. Data cleaning\n",
    "df = df.dropna(subset=['标题']).drop_duplicates(subset=['标题']).reset_index(drop=True)\n",
    "print(f\"Valid data volume after cleaning: {len(df)}\")\n",
    "\n",
    "# 3. Load stopwords and sentiment dictionary\n",
    "stopwords = load_stopwords(STOPWORDS_PATH)\n",
    "sent_dict = load_sentiment_dict(DICT_PATH)\n",
    "\n",
    "# 4. Text preprocessing\n",
    "df['tokens'] = df['标题'].apply(lambda x: preprocess_text(x, stopwords))\n",
    "df = df[df['tokens'].apply(len) > 0].reset_index(drop=True)  # Filter empty tokenization results\n",
    "print(f\"Valid tokenized data volume: {len(df)}\")\n",
    "\n",
    "# 5. Generate pseudo-labels (replace random labels)\n",
    "df['label'] = df['tokens'].apply(lambda x: generate_pseudo_labels(x, sent_dict))\n",
    "print(\"Label distribution:\")\n",
    "print(df['label'].value_counts())\n",
    "\n",
    "\n",
    "# ---------------------- 5. Word Embedding Training ----------------------\n",
    "print(\"\\n===== Training Word2Vec Word Embeddings =====\")\n",
    "sentences = df['tokens'].tolist()\n",
    "w2v_model = Word2Vec(\n",
    "    sentences,\n",
    "    vector_size=EMBED_DIM,\n",
    "    window=5,\n",
    "    min_count=1,\n",
    "    workers=4,\n",
    "    sg=1  # Use Skip-gram model\n",
    ")\n",
    "vocab = w2v_model.wv.key_to_index\n",
    "word2idx = {word: idx+1 for idx, word in enumerate(vocab)}  # Index starts from 1 (0 is PAD)\n",
    "word2idx['<PAD>'] = 0\n",
    "print(f\"Vocabulary size: {len(word2idx)}\")\n",
    "\n",
    "\n",
    "# ---------------------- 6. Build Dataset (Incorporating Sentiment Features) ----------------------\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, df, word2idx, sent_dict, max_len):\n",
    "        # 1. Word index sequences\n",
    "        self.input_ids = [\n",
    "            [word2idx.get(word, 0) for word in tokens] + [0]*(max_len - len(tokens)) \n",
    "            if len(tokens) < max_len else [word2idx.get(word, 0) for word in tokens[:max_len]]\n",
    "            for tokens in df['tokens']\n",
    "        ]\n",
    "        self.input_ids = torch.LongTensor(self.input_ids)\n",
    "        \n",
    "        # 2. Sentiment statistical features (number of positive words, total frequency of negative words)\n",
    "        self.pos_counts = torch.FloatTensor([\n",
    "            sum(1 for word in tokens if sent_dict.get(word, (None,))[0] == 'positive')\n",
    "            for tokens in df['tokens']\n",
    "        ]).unsqueeze(1)  # Shape: (n, 1)\n",
    "        \n",
    "        self.neg_weights = torch.FloatTensor([\n",
    "            sum(sent_dict[word][1] for word in tokens if sent_dict.get(word, (None,))[0] == 'negative')\n",
    "            for tokens in df['tokens']\n",
    "        ]).unsqueeze(1)  # Shape: (n, 1)\n",
    "        \n",
    "        # 3. Labels\n",
    "        self.labels = torch.LongTensor(df['label'].values)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.input_ids[idx],\n",
    "            'pos_count': self.pos_counts[idx],\n",
    "            'neg_weight': self.neg_weights[idx],\n",
    "            'label': self.labels[idx]\n",
    "        }\n",
    "\n",
    "\n",
    "# ---------------------- 7. Define LSTM Model with Sentiment Features ----------------------\n",
    "class SentimentLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim, sent_feat_dim=2):\n",
    "        super().__init__()\n",
    "        # Word embedding layer (using pre-trained Word2Vec)\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        \n",
    "        # LSTM layer (bidirectional)\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embed_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=2,\n",
    "            bidirectional=True,\n",
    "            batch_first=True,\n",
    "            dropout=0.2\n",
    "        )\n",
    "        \n",
    "        # Sentiment feature processing layer (map 2D sentiment features to LSTM output dimension)\n",
    "        self.sent_fc = nn.Linear(sent_feat_dim, hidden_dim * 2)  # Bidirectional LSTM output dimension is hidden_dim*2\n",
    "        \n",
    "        # Classification head (fuse LSTM features and sentiment features)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2 + hidden_dim * 2, 128),  # Concatenate LSTM output and sentiment features\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Word embedding: (batch_size, seq_len, embed_dim)\n",
    "        embeds = self.embedding(x['input_ids'])\n",
    "        \n",
    "        # LSTM processing: out shape (batch_size, seq_len, hidden_dim*2)\n",
    "        out, _ = self.lstm(embeds)\n",
    "        lstm_feat = out[:, -1, :]  # Take hidden state of the last time step\n",
    "        \n",
    "        # Sentiment feature processing: (batch_size, 2) → (batch_size, hidden_dim*2)\n",
    "        sent_feat = torch.cat([x['pos_count'], x['neg_weight']], dim=1)\n",
    "        sent_feat = self.sent_fc(sent_feat)\n",
    "        \n",
    "        # Fuse features and classify\n",
    "        combined_feat = torch.cat([lstm_feat, sent_feat], dim=1)\n",
    "        return self.classifier(combined_feat)\n",
    "\n",
    "\n",
    "# ---------------------- 8. Model Training and Validation ----------------------\n",
    "# 1. Split into training and validation sets (stratified sampling)\n",
    "print(\"\\n===== Preparing Training Data =====\")\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "train_idx, val_idx = next(sss.split(df, df['label']))\n",
    "train_df = df.iloc[train_idx]\n",
    "val_df = df.iloc[val_idx]\n",
    "\n",
    "# 2. Create datasets and data loaders\n",
    "train_dataset = SentimentDataset(train_df, word2idx, sent_dict, MAX_LEN)\n",
    "val_dataset = SentimentDataset(val_df, word2idx, sent_dict, MAX_LEN)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# 3. Initialize model and training configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = SentimentLSTM(\n",
    "    vocab_size=len(word2idx),\n",
    "    embed_dim=EMBED_DIM,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    output_dim=3  # Three categories: neutral, negative, positive\n",
    ").to(device)\n",
    "\n",
    "# Load pre-trained word embeddings (optional)\n",
    "embedding_matrix = np.zeros((len(word2idx), EMBED_DIM))\n",
    "for word, idx in word2idx.items():\n",
    "    if word in w2v_model.wv:\n",
    "        embedding_matrix[idx] = w2v_model.wv[word]\n",
    "model.embedding.weight.data.copy_(torch.from_numpy(embedding_matrix))\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)  # Reduce learning rate\n",
    "\n",
    "# 4. Training loop (with early stopping)\n",
    "print(\"\\n===== Starting Training =====\")\n",
    "best_val_acc = 0.0\n",
    "no_improvement = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    with tqdm(train_loader, unit='batch') as tepoch:\n",
    "        for batch in tepoch:\n",
    "            tepoch.set_description(f\"Epoch {epoch+1}\")\n",
    "            \n",
    "            # Load data to device\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            pos_count = batch['pos_count'].to(device)\n",
    "            neg_weight = batch['neg_weight'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            # Forward propagation + backward optimization\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model({'input_ids': input_ids, 'pos_count': pos_count, 'neg_weight': neg_weight})\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_train_loss += loss.item()\n",
    "            tepoch.set_postfix(loss=loss.item())\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    total_val_correct = 0\n",
    "    total_val_samples = 0\n",
    "    val_preds = []\n",
    "    val_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            pos_count = batch['pos_count'].to(device)\n",
    "            neg_weight = batch['neg_weight'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            outputs = model({'input_ids': input_ids, 'pos_count': pos_count, 'neg_weight': neg_weight})\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            \n",
    "            total_val_correct += (preds == labels).sum().item()\n",
    "            total_val_samples += labels.size(0)\n",
    "            val_preds.extend(preds.cpu().numpy())\n",
    "            val_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    val_acc = total_val_correct / total_val_samples\n",
    "    train_loss = total_train_loss / len(train_loader)\n",
    "    \n",
    "    # Calculate precision, recall, F1 score\n",
    "    if total_val_samples > 0:\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "            val_labels, val_preds, average=None, labels=[0, 1, 2]\n",
    "        )\n",
    "        # Calculate micro-average and macro-average\n",
    "        precision_micro, recall_micro, f1_micro, _ = precision_recall_fscore_support(\n",
    "            val_labels, val_preds, average='micro', labels=[0, 1, 2]\n",
    "        )\n",
    "        precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(\n",
    "            val_labels, val_preds, average='macro', labels=[0, 1, 2]\n",
    "        )\n",
    "        \n",
    "        # Print metrics\n",
    "        print(f\"Epoch {epoch+1}: Train Loss={train_loss:.4f}, Val Acc={val_acc:.4f}\")\n",
    "        print(f\"  Per-class metrics (0: neutral, 1: negative, 2: positive):\")\n",
    "        print(f\"  Precision: {precision[0]:.4f}, {precision[1]:.4f}, {precision[2]:.4f}\")\n",
    "        print(f\"  Recall: {recall[0]:.4f}, {recall[1]:.4f}, {recall[2]:.4f}\")\n",
    "        print(f\"  F1-score: {f1[0]:.4f}, {f1[1]:.4f}, {f1[2]:.4f}\")\n",
    "        print(f\"  Micro-average: Precision={precision_micro:.4f}, Recall={recall_micro:.4f}, F1={f1_micro:.4f}\")\n",
    "        print(f\"  Macro-average: Precision={precision_macro:.4f}, Recall={recall_macro:.4f}, F1={f1_macro:.4f}\")\n",
    "    else:\n",
    "        print(f\"Epoch {epoch+1}: Train Loss={train_loss:.4f}, Val Acc={val_acc:.4f}\")\n",
    "        print(\"  Validation set has 0 samples, cannot calculate classification metrics\")\n",
    "\n",
    "    # Early stopping check\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        no_improvement = 0\n",
    "        torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
    "        print(f\"Model saved (validation accuracy improved to {val_acc:.4f})\")\n",
    "    else:\n",
    "        no_improvement += 1\n",
    "        if no_improvement >= EARLY_STOPPING_PATIENCE:\n",
    "            print(f\"Early stopping triggered: validation accuracy did not improve for {EARLY_STOPPING_PATIENCE} consecutive epochs\")\n",
    "            break\n",
    "\n",
    "print(f\"\\nTraining completed, best validation accuracy: {best_val_acc:.4f}\")\n",
    "\n",
    "\n",
    "# ---------------------- 9. Sentiment Prediction and Result Output ----------------------\n",
    "def predict_sentiment(titles, model, word2idx, sent_dict, max_len, device):\n",
    "    \"\"\"Predict sentiment for new titles\"\"\"\n",
    "    model.eval()\n",
    "    scores = []\n",
    "    with torch.no_grad():\n",
    "        for title in tqdm(titles, desc=\"Predicting\"):\n",
    "            # Preprocessing\n",
    "            tokens = preprocess_text(title, stopwords)\n",
    "            input_ids = [word2idx.get(word, 0) for word in tokens]\n",
    "            input_ids = input_ids[:max_len] + [0]*(max_len - len(input_ids)) if len(input_ids) < max_len else input_ids[:max_len]\n",
    "            input_ids = torch.LongTensor([input_ids]).to(device)\n",
    "            \n",
    "            # Calculate sentiment features\n",
    "            pos_count = torch.FloatTensor([[sum(1 for word in tokens if sent_dict.get(word, (None,))[0] == 'positive')]]).to(device)\n",
    "            neg_weight = torch.FloatTensor([[sum(sent_dict[word][1] for word in tokens if sent_dict.get(word, (None,))[0] == 'negative')]]).to(device)\n",
    "            \n",
    "            # Model prediction\n",
    "            outputs = model({'input_ids': input_ids, 'pos_count': pos_count, 'neg_weight': neg_weight})\n",
    "            probs = torch.softmax(outputs, dim=1).cpu().numpy()[0]\n",
    "            scores.append(probs[2] - probs[1])  # Positive probability - negative probability\n",
    "    return scores\n",
    "\n",
    "# Predict sentiment for original data\n",
    "df['score'] = predict_sentiment(df['标题'].tolist(), model, word2idx, sent_dict, MAX_LEN, device)\n",
    "\n",
    "# Keep 4 decimal places\n",
    "df['score'] = df['score'].round(4)  \n",
    "\n",
    "# Merge with futures data\n",
    "print(\"\\n===== Merging Futures Data =====\")\n",
    "# Read futures data\n",
    "futures_data = pd.read_csv(f'data/{KEYWORD}.csv', index_col=0)  \n",
    "# Extract date and convert to string\n",
    "futures_data['date'] = pd.to_datetime(futures_data['time']).dt.date.astype(str)\n",
    "futures_data = futures_data.drop(columns=['time'])  # Remove original time column\n",
    "# Calculate daily sentiment scores\n",
    "daily_score = df.groupby(pd.to_datetime(df['最后更新时间']).dt.date.astype(str))['score'].mean().reset_index()\n",
    "daily_score.columns = ['date', 'score']\n",
    "# Merge data and adjust column order\n",
    "final_data = futures_data.merge(daily_score, on='date', how='left').fillna(0)\n",
    "# Adjust column order\n",
    "target_columns = ['date', 'open', 'high', 'low', 'close', 'volume', 'score']\n",
    "# target_columns = ['date', 'code', 'open', 'high', 'low', 'close', 'volume', 'amount', 'open_interest', 'score']\n",
    "final_data = final_data[target_columns]\n",
    "# Save results\n",
    "final_data.to_csv(OUTPUT_PATH, index=False)\n",
    "print(f\"✅ Final results saved to {OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6266d0",
   "metadata": {},
   "source": [
    "# Financial Time Series Prediction with DQN-Enhanced Hybrid Transformer and Sentiment Features (DQN-HTS-EF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e397236",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from collections import deque\n",
    "from datetime import datetime\n",
    "\n",
    "# ---------------------- 1. 基础配置与数据加载 ----------------------\n",
    "# 设备配置（GPU优先）\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 数据参数配置（需确保data文件夹存在）\n",
    "KEYWORD = \"CSI100\"  # 替换为你的数据关键词（如股票名称）\n",
    "data_path = f'data/{KEYWORD}_data_with_sentiment_scores.csv'\n",
    "time_steps = 7  # 时间序列窗口长度（用前7天预测后1天）\n",
    "y_col = 1  # 预测目标：收盘价（对应net_df中'close'列的索引）\n",
    "\n",
    "# 读取与预处理数据\n",
    "data = pd.read_csv(data_path)\n",
    "dates = pd.to_datetime(data['date'])  # 日期列（用于后续可视化）\n",
    "net_df = data[['open', 'close', 'high', 'low', 'volume', 'score']]  # 特征列（含情感分数）\n",
    "\n",
    "# 缺失值填充（前向填充）与标准化（Min-Max到[0,1]）\n",
    "data.fillna(method='ffill', inplace=True)\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(net_df)\n",
    "features = scaled_data  # 标准化后的特征矩阵\n",
    "\n",
    "\n",
    "# ---------------------- 2. 时间序列数据集构造 ----------------------\n",
    "def create_dataset(X, y_col, time_steps):\n",
    "    \"\"\"\n",
    "    构造时间序列训练/测试集\n",
    "    参数：\n",
    "        X: 标准化后的特征矩阵 (n_samples, n_features)\n",
    "        y_col: 目标列索引（收盘价）\n",
    "        time_steps: 时间窗口长度\n",
    "    返回：\n",
    "        Xs: 输入特征 (n_samples-time_steps, time_steps, n_features)\n",
    "        ys: 目标值 (n_samples-time_steps,)\n",
    "    \"\"\"\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - time_steps):\n",
    "        Xs.append(X[i:(i + time_steps)])  # 截取前time_steps步作为输入\n",
    "        ys.append(X[i + time_steps, y_col])  # 第time_steps+1步的收盘价作为目标\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "# 生成时间序列数据\n",
    "X, y = create_dataset(features, y_col, time_steps)\n",
    "\n",
    "# 数据集拆分（训练集22.2%，测试集77.8%，即2:7比例）\n",
    "test_ratio = 7 / 9\n",
    "test_size = int(len(X) * test_ratio)\n",
    "X_train, X_test = X[:-test_size], X[-test_size:]  # 输入特征拆分\n",
    "y_train, y_test = y[:-test_size], y[-test_size:]  # 目标值拆分\n",
    "\n",
    "# 提取测试集对应的日期（用于后续可视化和结果保存）\n",
    "test_dates = dates[time_steps + len(X_train):time_steps + len(X_train) + len(X_test)]\n",
    "test_dates = test_dates.dt.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# 转换为PyTorch张量（适配Transformer模型）\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32).to(device)\n",
    "\n",
    "\n",
    "# ---------------------- 3. 模型定义（Transformer + SVR） ----------------------\n",
    "# 3.1 Transformer基础模块（多头注意力+前馈网络）\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, input_dim, num_heads, ff_dim, dropout=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(input_dim, num_heads, batch_first=True)  # 多头自注意力\n",
    "        self.feed_forward = nn.Sequential(  # 前馈网络\n",
    "            nn.Linear(input_dim, ff_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(ff_dim, input_dim)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(input_dim)  # 层归一化1\n",
    "        self.norm2 = nn.LayerNorm(input_dim)  # 层归一化2\n",
    "        self.dropout = nn.Dropout(dropout)  #  dropout正则化\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 自注意力层\n",
    "        attn_output, _ = self.self_attn(x, x, x)  # (batch_size, time_steps, input_dim)\n",
    "        x = self.norm1(x + self.dropout(attn_output))  # 残差连接+归一化\n",
    "        # 前馈网络层\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))  # 残差连接+归一化\n",
    "        return x\n",
    "\n",
    "# 3.2 单一层Transformer模型\n",
    "class SingleTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, num_heads, ff_dim, dropout=0.1):\n",
    "        super(SingleTransformer, self).__init__()\n",
    "        self.transformer_block = TransformerBlock(input_dim, num_heads, ff_dim, dropout)\n",
    "        self.fc = nn.Linear(input_dim, 1)  # 输出层（预测收盘价）\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.transformer_block(x)  # (batch_size, time_steps, input_dim)\n",
    "        x = x[:, -1, :]  # 取最后一个时间步的特征（用于预测）\n",
    "        return self.fc(x).squeeze(-1)  # 压缩维度（batch_size,）\n",
    "\n",
    "# 3.3 多层Transformer模型（2层）\n",
    "class MultiLayerTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, num_heads, ff_dim, num_layers=2, dropout=0.1):\n",
    "        super(MultiLayerTransformer, self).__init__()\n",
    "        self.layers = nn.ModuleList([  # 堆叠Transformer层\n",
    "            TransformerBlock(input_dim, num_heads, ff_dim, dropout) \n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.fc = nn.Linear(input_dim, 1)  # 输出层\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)  # 逐层传递\n",
    "        x = x[:, -1, :]  # 取最后一个时间步\n",
    "        return self.fc(x).squeeze(-1)\n",
    "\n",
    "# 3.4 双向Transformer模型（前向+后向特征融合）\n",
    "class BidirectionalTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, num_heads, ff_dim, dropout=0.1):\n",
    "        super(BidirectionalTransformer, self).__init__()\n",
    "        self.forward_layer = TransformerBlock(input_dim, num_heads, ff_dim, dropout)  # 前向层\n",
    "        self.backward_layer = TransformerBlock(input_dim, num_heads, ff_dim, dropout)  # 后向层\n",
    "        self.fc = nn.Linear(2 * input_dim, 1)  # 融合前向+后向特征（维度翻倍）\n",
    "\n",
    "    def forward(self, x):\n",
    "        forward_out = self.forward_layer(x)  # 前向传播\n",
    "        backward_out = self.backward_layer(x.flip(1))  # 后向传播（翻转时间步）\n",
    "        # 融合最后一个时间步的前向+后向特征\n",
    "        combined = torch.cat([forward_out[:, -1], backward_out[:, -1]], dim=-1)\n",
    "        return self.fc(combined).squeeze(-1)\n",
    "\n",
    "# 3.5 早停机制（防止过拟合，无验证集时自动跳过）\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10, min_delta=0):\n",
    "        self.patience = 5\n",
    "        self.min_delta = 1e-3\n",
    "        self.best_loss = None  # 最佳验证损失\n",
    "        self.counter = 0  # 无改进计数器\n",
    "        self.early_stop = False  # 早停标志\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss  # 初始化最佳损失\n",
    "        elif val_loss >= self.best_loss - self.min_delta:\n",
    "            self.counter += 1  # 无改进，计数器+1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True  # 触发早停\n",
    "        else:\n",
    "            self.best_loss = val_loss  # 更新最佳损失\n",
    "            self.counter = 0  # 重置计数器\n",
    "\n",
    "\n",
    "# ---------------------- 4. 模型训练（Transformer + SVR） ----------------------\n",
    "# 4.1 Transformer训练函数\n",
    "def train_model(model, X_train, y_train, X_val=None, y_val=None, epochs=200, lr=0.001, batch_size=32, patience=10):\n",
    "    \"\"\"\n",
    "    训练Transformer模型（支持验证集早停）\n",
    "    参数：\n",
    "        model: 待训练的Transformer模型\n",
    "        X_train/y_train: 训练集\n",
    "        X_val/y_val: 验证集（可选）\n",
    "        epochs: 最大训练轮次\n",
    "        lr: 学习率\n",
    "        batch_size: 批次大小\n",
    "        patience: 早停等待次数\n",
    "    返回：\n",
    "        训练后的模型\n",
    "    \"\"\"\n",
    "    criterion = nn.MSELoss()  # 损失函数（均方误差，适合回归任务）\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)  # 优化器（Adam）\n",
    "    # 仅当有验证集时初始化早停\n",
    "    early_stopping = EarlyStopping(patience=patience) if (X_val is not None and y_val is not None) else None\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()  # 训练模式（启用dropout）\n",
    "        total_loss = 0  # 累计训练损失\n",
    "\n",
    "        # 批次训练\n",
    "        for i in range(0, len(X_train), batch_size):\n",
    "            batch_x = X_train[i:i+batch_size]  # 批次输入\n",
    "            batch_y = y_train[i:i+batch_size]  # 批次目标\n",
    "\n",
    "            # 梯度清零→前向传播→计算损失→反向传播→参数更新\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()  # 累加损失\n",
    "\n",
    "        # 验证集早停（仅当有验证集时执行）\n",
    "        if early_stopping is not None and X_val is not None and y_val is not None:\n",
    "            model.eval()  # 评估模式（关闭dropout）\n",
    "            with torch.no_grad():  # 禁用梯度计算（节省内存）\n",
    "                val_outputs = model(X_val)\n",
    "                val_loss = criterion(val_outputs, y_val).item()\n",
    "\n",
    "            early_stopping(val_loss)\n",
    "            if early_stopping.early_stop:\n",
    "                print(f\"Early stopping at epoch {epoch+1} (val_loss: {val_loss:.6f})\")\n",
    "                break\n",
    "\n",
    "    return model\n",
    "\n",
    "# 4.2 初始化并训练3个Transformer模型\n",
    "# 模型超参数（经网格搜索优化后的最佳参数）\n",
    "single_best = {'num_heads': 2, 'ff_dim': 32}  # 单一层Transformer参数\n",
    "multi_best = {'num_heads': 2, 'ff_dim': 64}   # 多层Transformer参数\n",
    "bi_best = {'num_heads': 2, 'ff_dim': 32}      # 双向Transformer参数\n",
    "input_dim = features.shape[1]  # 输入特征维度（6：open/close/high/low/volume/score）\n",
    "\n",
    "# 初始化模型（部署到指定设备）\n",
    "single_transformer = SingleTransformer(input_dim, **single_best).to(device)\n",
    "multi_layer_transformer = MultiLayerTransformer(input_dim, **multi_best, num_layers=2).to(device)\n",
    "bidirectional_transformer = BidirectionalTransformer(input_dim, **bi_best).to(device)\n",
    "\n",
    "# 训练模型（无验证集，早停自动跳过）\n",
    "print(\"\\nTraining Single-layer Transformer...\")\n",
    "single_transformer = train_model(single_transformer, X_train, y_train, X_val=None, y_val=None, patience=15)\n",
    "\n",
    "print(\"Training Multi-layer Transformer...\")\n",
    "multi_layer_transformer = train_model(multi_layer_transformer, X_train, y_train, X_val=None, y_val=None, patience=15)\n",
    "\n",
    "print(\"Training Bidirectional Transformer...\")\n",
    "bidirectional_transformer = train_model(bidirectional_transformer, X_train, y_train, X_val=None, y_val=None, patience=15)\n",
    "\n",
    "# 4.3 训练SVR模型（传统机器学习基线）\n",
    "# SVR输入需展平（时间序列窗口→1维特征）\n",
    "X_train_svr = X_train.cpu().numpy().reshape(-1, time_steps * input_dim)  # (n_samples, time_steps*input_dim)\n",
    "y_train_svr = y_train.cpu().numpy()  # 目标值转回numpy\n",
    "\n",
    "# 网格搜索优化SVR超参数（C：正则化强度，gamma：RBF核带宽）\n",
    "param_grid = {'C': [0.1, 1, 10], 'gamma': [0.001, 0.01]}\n",
    "grid = GridSearchCV(SVR(kernel='rbf'), param_grid, cv=3, n_jobs=-1)  # 3折交叉验证，多线程加速\n",
    "grid.fit(X_train_svr, y_train_svr)\n",
    "svr = grid.best_estimator_  # 最佳SVR模型\n",
    "print(f\"Best SVR params: {grid.best_params_}\")\n",
    "\n",
    "\n",
    "# ---------------------- 5. 模型预测与损失计算（MCS输入准备） ----------------------\n",
    "def get_predictions(model, X_test, time_steps, input_dim):\n",
    "    \"\"\"\n",
    "    获取模型预测结果（适配Transformer和SVR）\n",
    "    参数：\n",
    "        model: 训练后的模型（Transformer或SVR）\n",
    "        X_test: 测试集输入\n",
    "        time_steps: 时间窗口长度\n",
    "        input_dim: 输入特征维度\n",
    "    返回：\n",
    "        预测结果（numpy数组）\n",
    "    \"\"\"\n",
    "    if isinstance(model, torch.nn.Module):  # Transformer模型（PyTorch）\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            return model(X_test).cpu().numpy()  # 转回CPU并转为numpy\n",
    "    else:  # SVR模型（sklearn）\n",
    "        X_test_flat = X_test.cpu().numpy().reshape(-1, time_steps * input_dim)  # 展平输入\n",
    "        return model.predict(X_test_flat)\n",
    "\n",
    "# 5.1 获取所有模型的测试集预测（标准化后）\n",
    "single_pred = get_predictions(single_transformer, X_test, time_steps, input_dim)\n",
    "multi_pred = get_predictions(multi_layer_transformer, X_test, time_steps, input_dim)\n",
    "bi_pred = get_predictions(bidirectional_transformer, X_test, time_steps, input_dim)\n",
    "svr_pred = get_predictions(svr, X_test, time_steps, input_dim)\n",
    "\n",
    "# 5.2 逆标准化（将预测值/真实值转回原始价格尺度）\n",
    "def inverse_transform_pred(pred, scaler, y_col, input_dim):\n",
    "    \"\"\"\n",
    "    逆标准化函数（Min-Max逆变换）\n",
    "    参数：\n",
    "        pred: 标准化后的预测值/真实值\n",
    "        scaler: 训练好的MinMaxScaler\n",
    "        y_col: 目标列索引\n",
    "        input_dim: 输入特征维度\n",
    "    返回：\n",
    "        原始尺度的值\n",
    "    \"\"\"\n",
    "    dummy = np.zeros((len(pred), input_dim))  # 构造与特征矩阵同维度的虚拟矩阵\n",
    "    dummy[:, y_col] = pred  # 仅目标列填入值，其他列填0\n",
    "    return scaler.inverse_transform(dummy)[:, y_col]  # 逆变换后取目标列\n",
    "\n",
    "# 逆标准化所有结果（原始价格尺度）\n",
    "y_test_inverse = inverse_transform_pred(y_test.cpu().numpy(), scaler, y_col, input_dim)\n",
    "single_pred_inverse = inverse_transform_pred(single_pred, scaler, y_col, input_dim)\n",
    "multi_pred_inverse = inverse_transform_pred(multi_pred, scaler, y_col, input_dim)\n",
    "bi_pred_inverse = inverse_transform_pred(bi_pred, scaler, y_col, input_dim)\n",
    "svr_pred_inverse = inverse_transform_pred(svr_pred, scaler, y_col, input_dim)\n",
    "\n",
    "# 5.3 计算每个时间点的模型损失（MAE：平均绝对误差，对异常值更稳健）\n",
    "def calculate_timewise_loss(y_true, y_pred, loss_type='mae'):\n",
    "    \"\"\"\n",
    "    计算每个时间点的损失（逐样本损失）\n",
    "    参数：\n",
    "        y_true: 真实值（原始尺度）\n",
    "        y_pred: 预测值（原始尺度）\n",
    "        loss_type: 损失类型（'mae' 或 'mse'）\n",
    "    返回：\n",
    "        逐时间点损失数组（长度=测试集样本数）\n",
    "    \"\"\"\n",
    "    if loss_type == 'mae':\n",
    "        return np.abs(y_true - y_pred)  # 逐点MAE损失\n",
    "    elif loss_type == 'mse':\n",
    "        return (y_true - y_pred) ** 2  # 逐点MSE损失\n",
    "    else:\n",
    "        raise ValueError(\"loss_type must be 'mae' or 'mse'\")\n",
    "\n",
    "# 定义模型名称与预测结果映射（便于后续迭代）\n",
    "models = {\n",
    "    'Single-Transformer': single_pred_inverse,\n",
    "    'Multi-Transformer': multi_pred_inverse,\n",
    "    'Bi-Transformer': bi_pred_inverse,\n",
    "    'SVR': svr_pred_inverse\n",
    "}\n",
    "\n",
    "# 计算每个模型的逐时间点损失（选用MAE，对异常值更稳健）\n",
    "timewise_loss = {}\n",
    "for name, pred in models.items():\n",
    "    timewise_loss[name] = calculate_timewise_loss(y_test_inverse, pred, loss_type='mae')\n",
    "\n",
    "# 将损失转换为矩阵格式（行=时间点，列=模型）\n",
    "loss_matrix = np.column_stack([timewise_loss[name] for name in models.keys()])\n",
    "model_names = list(models.keys())  # 模型名称列表（与loss_matrix列顺序对应）\n",
    "\n",
    "\n",
    "# ---------------------- 6. MCS检验（模型信度集筛选） ----------------------\n",
    "def bootstrap_pvalue(loss_a, loss_b, n_bootstrap=1000, alpha=0.05):\n",
    "    \"\"\"\n",
    "    自助法（Bootstrap）计算成对模型损失差异的p值\n",
    "    参数：\n",
    "        loss_a: 模型A的逐时间点损失\n",
    "        loss_b: 模型B的逐时间点损失\n",
    "        n_bootstrap: 自助抽样次数（越大越稳健，建议≥1000）\n",
    "        alpha: 显著性水平（默认0.05）\n",
    "    返回：\n",
    "        p值: 原假设（A与B损失无差异）的概率\n",
    "        is_significant: 是否存在显著差异（p < alpha）\n",
    "    \"\"\"\n",
    "    n_samples = len(loss_a)\n",
    "    diff_obs = loss_a - loss_b  # 观测到的损失差异（A - B）\n",
    "    null_distribution = []  # 自助法生成的零假设分布\n",
    "\n",
    "    # 自助抽样：生成n_bootstrap个重采样样本\n",
    "    for _ in range(n_bootstrap):\n",
    "        # 有放回抽样（索引）\n",
    "        idx = np.random.choice(n_samples, size=n_samples, replace=True)\n",
    "        diff_boot = diff_obs[idx]  # 重采样的损失差异\n",
    "        null_distribution.append(np.mean(diff_boot))  # 计算重采样差异的均值\n",
    "\n",
    "    # 计算p值：零假设下，|重采样均值| ≥ |观测均值| 的比例\n",
    "    obs_mean = np.mean(diff_obs)\n",
    "    p_value = np.sum(np.abs(null_distribution) >= np.abs(obs_mean)) / n_bootstrap\n",
    "    return p_value, (p_value < alpha)\n",
    "\n",
    "def run_mcs(loss_matrix, model_names, n_bootstrap=1000, alpha=0.05):\n",
    "    \"\"\"\n",
    "    运行MCS检验，逐步剔除最差模型，获取模型信度集（MCS）\n",
    "    参数：\n",
    "        loss_matrix: 损失矩阵（行=时间点，列=模型）\n",
    "        model_names: 模型名称列表（与列顺序对应）\n",
    "        n_bootstrap: 自助抽样次数\n",
    "        alpha: 显著性水平\n",
    "    返回：\n",
    "        mcs_set: 模型信度集（最终无显著差异的模型）\n",
    "        elimination_order: 模型剔除顺序（从最差到次差）\n",
    "        p_value_history: 每次检验的p值记录\n",
    "    \"\"\"\n",
    "    # 初始化MCS候选集（包含所有模型）\n",
    "    current_candidates = list(range(len(model_names)))  # 用索引代表模型\n",
    "    elimination_order = []  # 记录剔除顺序（模型名称）\n",
    "    p_value_history = []    # 记录每次检验的p值\n",
    "\n",
    "    # 逐步剔除：直到候选集内无显著差异\n",
    "    while len(current_candidates) > 1:\n",
    "        n_candidates = len(current_candidates)\n",
    "        p_matrix = np.ones((n_candidates, n_candidates))  # p值矩阵（i,j：模型i vs 模型j）\n",
    "\n",
    "        # 计算所有成对模型的p值\n",
    "        for i in range(n_candidates):\n",
    "            for j in range(i+1, n_candidates):\n",
    "                # 获取两个模型的损失\n",
    "                loss_i = loss_matrix[:, current_candidates[i]]\n",
    "                loss_j = loss_matrix[:, current_candidates[j]]\n",
    "                # 计算p值\n",
    "                p_val, _ = bootstrap_pvalue(loss_i, loss_j, n_bootstrap, alpha)\n",
    "                p_matrix[i, j] = p_val\n",
    "                p_matrix[j, i] = p_val  # 对称矩阵\n",
    "\n",
    "        # 计算每个模型的\"最小p值\"（与其他所有模型比较的最小p值）\n",
    "        min_p_per_model = np.min(p_matrix, axis=1)\n",
    "        # 找到\"最小p值最小\"的模型（表现最差，最可能被剔除）\n",
    "        worst_idx_in_curr = np.argmin(min_p_per_model)\n",
    "        worst_model_idx = current_candidates[worst_idx_in_curr]\n",
    "        worst_model_name = model_names[worst_model_idx]\n",
    "        worst_p = min_p_per_model[worst_idx_in_curr]\n",
    "\n",
    "        # 记录结果\n",
    "        elimination_order.append(worst_model_name)\n",
    "        p_value_history.append(worst_p)\n",
    "\n",
    "        # 剔除最差模型\n",
    "        current_candidates.pop(worst_idx_in_curr)\n",
    "        print(f\"剔除模型：{worst_model_name}，p值：{worst_p:.4f}（<{alpha}，显著差异）\")\n",
    "\n",
    "    # 最终候选集即为模型信度集（MCS）\n",
    "    mcs_set = [model_names[idx] for idx in current_candidates]\n",
    "    print(f\"\\nMCS模型信度集（无显著差异的最佳模型）：{mcs_set}\")\n",
    "    return mcs_set, elimination_order, p_value_history\n",
    "\n",
    "# 运行MCS检验（自助抽样1000次，显著性水平0.05）\n",
    "print(\"\\n=== 开始MCS检验（逐步剔除最差模型） ===\")\n",
    "mcs_set, elimination_order, p_value_history = run_mcs(\n",
    "    loss_matrix=loss_matrix,\n",
    "    model_names=model_names,\n",
    "    n_bootstrap=1000,\n",
    "    alpha=0.05\n",
    ")\n",
    "\n",
    "\n",
    "# ---------------------- 7. 基于MCS信度集的组合预测 ----------------------\n",
    "def mcs_ensemble_pred(mcs_set, models, weight_type='equal'):\n",
    "    \"\"\"\n",
    "    基于MCS信度集的组合预测（等权重或损失反比权重）\n",
    "    参数：\n",
    "        mcs_set: MCS模型信度集（模型名称列表）\n",
    "        models: 所有模型的预测结果（{模型名: 预测数组}）\n",
    "        weight_type: 权重类型（'equal'：等权重，'inv_loss'：损失反比权重）\n",
    "    返回：\n",
    "        ensemble_pred: 组合预测结果\n",
    "        weights: 各模型的权重\n",
    "    \"\"\"\n",
    "    # 提取MCS集中模型的预测和损失\n",
    "    mcs_preds = [models[name] for name in mcs_set]\n",
    "    mcs_losses = [timewise_loss[name] for name in mcs_set]\n",
    "\n",
    "    if weight_type == 'equal':\n",
    "        # 等权重：每个模型权重=1/模型数量\n",
    "        n_mcs = len(mcs_set)\n",
    "        weights = np.ones(n_mcs) / n_mcs\n",
    "    elif weight_type == 'inv_loss':\n",
    "        # 损失反比权重：损失越小，权重越大（基于平均损失）\n",
    "        mean_losses = [np.mean(loss) for loss in mcs_losses]\n",
    "        inv_loss = 1 / np.array(mean_losses)  # 损失反比\n",
    "        weights = inv_loss / np.sum(inv_loss)  # 归一化\n",
    "    else:\n",
    "        raise ValueError(\"weight_type must be 'equal' or 'inv_loss'\")\n",
    "\n",
    "    # 计算组合预测（加权求和）\n",
    "    ensemble_pred = np.average(mcs_preds, weights=weights, axis=0)\n",
    "    # 打印权重信息\n",
    "    print(f\"\\nMCS组合预测权重（{weight_type}）：\")\n",
    "    for name, w in zip(mcs_set, weights):\n",
    "        print(f\"  {name}: {w:.4f}\")\n",
    "    return ensemble_pred, weights\n",
    "\n",
    "# 生成MCS组合预测（默认等权重，可切换为'inv_loss'）\n",
    "mcs_ensemble_pred, mcs_weights = mcs_ensemble_pred(\n",
    "    mcs_set=mcs_set,\n",
    "    models=models,\n",
    "    weight_type='equal'\n",
    ")\n",
    "\n",
    "\n",
    "# ---------------------- 8. 结果评估与可视化 ----------------------\n",
    "# 8.1 计算所有模型（含MCS组合）的评估指标\n",
    "def calculate_metrics(y_true, y_pred, name):\n",
    "    \"\"\"计算回归任务的核心指标：RMSE、MAE、R²、MAPE\"\"\"\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    # MAPE（避免除以0，用掩码处理）\n",
    "    mask = y_true != 0\n",
    "    if np.sum(mask) == 0:\n",
    "        mape = float('nan')\n",
    "    else:\n",
    "        mape = np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100\n",
    "    return {\n",
    "        'Model': name,\n",
    "        'MSE': mse,        \n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae,\n",
    "        'R²': r2,\n",
    "        'MAPE(%)': mape\n",
    "    }\n",
    "\n",
    "# 计算所有模型的指标\n",
    "metrics_list = []\n",
    "# 原始4个模型\n",
    "for name, pred in models.items():\n",
    "    metrics_list.append(calculate_metrics(y_test_inverse, pred, name))\n",
    "# MCS组合模型\n",
    "metrics_list.append(calculate_metrics(y_test_inverse, mcs_ensemble_pred, 'MCS-Ensemble'))\n",
    "\n",
    "# 转换为DataFrame便于查看\n",
    "metrics_df = pd.DataFrame(metrics_list)\n",
    "print(\"\\n=== 所有模型（含MCS组合）评估指标 ===\")\n",
    "print(metrics_df.round(4))\n",
    "\n",
    "# 8.2 可视化：真实值 vs 各模型预测值（含MCS组合）\n",
    "plt.figure(figsize=(14, 8))\n",
    "# 真实值（黑色实线，突出显示）\n",
    "plt.plot(test_dates, y_test_inverse, '-', color='black', linewidth=2.5, label='Actual Price', alpha=0.9)\n",
    "# 原始4个模型（虚线）\n",
    "colors = ['tab:blue', 'tab:green', 'tab:red', 'tab:orange']\n",
    "styles = ['--', '--', '--', '-.']\n",
    "for i, (name, pred) in enumerate(models.items()):\n",
    "    plt.plot(\n",
    "        test_dates, pred, styles[i], color=colors[i], \n",
    "        linewidth=1.5, label=name, alpha=0.7\n",
    "    )\n",
    "# MCS组合模型（紫色实线，加粗）\n",
    "plt.plot(\n",
    "    test_dates, mcs_ensemble_pred, '-', color='tab:purple', \n",
    "    linewidth=2.5, label='MCS-Ensemble', alpha=0.9\n",
    ")\n",
    "\n",
    "# 日期轴处理（避免过于密集，按月份显示）\n",
    "unique_months = []\n",
    "unique_indices = []\n",
    "month_set = set()\n",
    "for idx, date in enumerate(pd.to_datetime(test_dates)):\n",
    "    month_key = date.strftime('%Y-%m')\n",
    "    if month_key not in month_set:\n",
    "        unique_months.append(month_key)\n",
    "        unique_indices.append(idx)\n",
    "        month_set.add(month_key)\n",
    "plt.xticks(unique_indices, unique_months, rotation=45)\n",
    "\n",
    "# 图表标签与图例\n",
    "plt.xlabel('Date', fontsize=12)\n",
    "plt.ylabel('Price', fontsize=12)\n",
    "plt.title(f'Price Prediction Comparison (Corn) - MCS Ensemble', fontsize=14)\n",
    "plt.legend(fontsize=10, loc='best')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ---------------------- 9. 结果保存（便于后续分析） ----------------------\n",
    "# 9.1 保存所有预测结果（含MCS组合）\n",
    "prediction_data = {\n",
    "    'Date': test_dates,\n",
    "    'Actual_Price': y_test_inverse,\n",
    "    'Single-Transformer': single_pred_inverse,\n",
    "    'Multi-Transformer': multi_pred_inverse,\n",
    "    'Bi-Transformer': bi_pred_inverse,\n",
    "    'SVR': svr_pred_inverse,\n",
    "    'MCS-Ensemble': mcs_ensemble_pred\n",
    "}\n",
    "prediction_df = pd.DataFrame(prediction_data)\n",
    "prediction_df.to_csv(f'data/{KEYWORD}_mcs_prediction_results.csv', index=False)\n",
    "print(f\"\\n预测结果已保存至：data/{KEYWORD}_mcs_prediction_results.csv\")\n",
    "\n",
    "# 9.2 保存MCS检验结果\n",
    "mcs_result_data = {\n",
    "    'MCS_Set': [', '.join(mcs_set)],\n",
    "    'Elimination_Order': [', '.join(elimination_order)],\n",
    "    'Elimination_P_Values': [', '.join([f'{p:.4f}' for p in p_value_history])],\n",
    "    'MCS_Weights': [', '.join([f'{name}:{w:.4f}' for name, w in zip(mcs_set, mcs_weights)])]\n",
    "}\n",
    "mcs_result_df = pd.DataFrame(mcs_result_data)\n",
    "mcs_result_df.to_csv(f'data/{KEYWORD}_mcs_test_results.csv', index=False)\n",
    "print(f\"MCS检验结果已保存至：data/{KEYWORD}_mcs_test_results.csv\")\n",
    "\n",
    "# 9.3 保存评估指标\n",
    "metrics_df.to_csv(f'data/{KEYWORD}_model_metrics.csv', index=False)\n",
    "print(f\"模型评估指标已保存至：data/{KEYWORD}_model_metrics.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3183f292",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import random\n",
    "from collections import deque\n",
    "from datetime import datetime\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 需先定义KEYWORD（示例：CSI100，根据你的数据修改）\n",
    "KEYWORD = \"Corn\"\n",
    "\n",
    "# Read data\n",
    "data = pd.read_csv(f'data/{KEYWORD}_data_with_sentiment_scores.csv')\n",
    "dates = pd.to_datetime(data['date'])\n",
    "net_df = data[['open', 'close', 'high', 'low', 'volume', 'score']]\n",
    "\n",
    "# Data preprocessing\n",
    "data.fillna(method='ffill', inplace=True)\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(net_df)\n",
    "features = scaled_data\n",
    "y_col = 1  # 预测目标：收盘价（close）\n",
    "\n",
    "# Create time series dataset（不变）\n",
    "def create_dataset(X, y_col, time_steps=1):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - time_steps):\n",
    "        Xs.append(X[i:(i + time_steps)])\n",
    "        ys.append(X[i + time_steps, y_col])\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "time_steps = 7\n",
    "X, y = create_dataset(features, y_col, time_steps)\n",
    "\n",
    "# ★ 1. 调整训练集/测试集比例为2:7（训练集22.2%，测试集77.8%）\n",
    "test_ratio = 7 / 9  # 2:7 → 测试集占7份\n",
    "test_size = int(len(X) * test_ratio)\n",
    "X_train, X_test = X[:-test_size], X[-test_size:]\n",
    "y_train, y_test = y[:-test_size], y[-test_size:]\n",
    "\n",
    "# Extract test set dates（不变）\n",
    "test_dates = dates[time_steps + len(X_train):time_steps + len(X_train) + len(X_test)]\n",
    "test_dates = test_dates.dt.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Convert to PyTorch tensors（不变）\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32).to(device)\n",
    "\n",
    "# ---------------------- Transformer模型定义（全部不变）----------------------\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, input_dim, num_heads, ff_dim, dropout=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(input_dim, num_heads, batch_first=True)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(input_dim, ff_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(ff_dim, input_dim)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(input_dim)\n",
    "        self.norm2 = nn.LayerNorm(input_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_output, _ = self.self_attn(x, x, x)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        return x\n",
    "\n",
    "class SingleTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, num_heads, ff_dim, dropout=0.1):\n",
    "        super(SingleTransformer, self).__init__()\n",
    "        self.transformer_block = TransformerBlock(input_dim, num_heads, ff_dim, dropout)\n",
    "        self.fc = nn.Linear(input_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.transformer_block(x)\n",
    "        x = x[:, -1, :]\n",
    "        return self.fc(x).squeeze(-1)\n",
    "\n",
    "class MultiLayerTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, num_heads, ff_dim, num_layers=2, dropout=0.1):\n",
    "        super(MultiLayerTransformer, self).__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerBlock(input_dim, num_heads, ff_dim, dropout) \n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.fc = nn.Linear(input_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        x = x[:, -1, :]\n",
    "        return self.fc(x).squeeze(-1)\n",
    "\n",
    "class BidirectionalTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, num_heads, ff_dim, dropout=0.1):\n",
    "        super(BidirectionalTransformer, self).__init__()\n",
    "        self.forward_layer = TransformerBlock(input_dim, num_heads, ff_dim, dropout)\n",
    "        self.backward_layer = TransformerBlock(input_dim, num_heads, ff_dim, dropout)\n",
    "        self.fc = nn.Linear(2*input_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        forward_out = self.forward_layer(x)\n",
    "        backward_out = self.backward_layer(x.flip(1))\n",
    "        combined = torch.cat([forward_out[:, -1], backward_out[:, -1]], dim=-1)\n",
    "        return self.fc(combined).squeeze(-1)\n",
    "\n",
    "# Early stopping机制\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.best_loss = None\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "        elif val_loss >= self.best_loss - self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "\n",
    "# ★ 2. 修改训练函数\n",
    "def train_model(model, X_train, y_train, X_val=None, y_val=None, epochs=200, lr=0.001, batch_size=32, patience=10):\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    early_stopping = EarlyStopping(patience=patience) if (X_val is not None and y_val is not None) else None\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for i in range(0, len(X_train), batch_size):\n",
    "            batch_x = X_train[i:i+batch_size]\n",
    "            batch_y = y_train[i:i+batch_size]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        # 无验证集，跳过验证和早停\n",
    "        if early_stopping is not None and X_val is not None and y_val is not None:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_outputs = model(X_val)\n",
    "                val_loss = criterion(val_outputs, y_val).item()\n",
    "            \n",
    "            early_stopping(val_loss)\n",
    "            if early_stopping.early_stop:\n",
    "                break\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "X_train_split, y_train_split = X_train, y_train\n",
    "X_val, y_val = None, None\n",
    "\n",
    "# Model parameters\n",
    "single_best = {'num_heads': 2, 'ff_dim': 32}\n",
    "multi_best = {'num_heads': 2, 'ff_dim': 64}\n",
    "bi_best = {'num_heads': 2, 'ff_dim': 32}\n",
    "\n",
    "# 初始化并训练Transformer模型\n",
    "single_transformer = SingleTransformer(features.shape[1], **single_best).to(device)\n",
    "multi_layer_transformer = MultiLayerTransformer(features.shape[1],** multi_best, num_layers=2).to(device)\n",
    "bidirectional_transformer = BidirectionalTransformer(features.shape[1], **bi_best).to(device)\n",
    "\n",
    "single_transformer = train_model(single_transformer, X_train_split, y_train_split, X_val, y_val, patience=15)\n",
    "multi_layer_transformer = train_model(multi_layer_transformer, X_train_split, y_train_split, X_val, y_val, patience=15)\n",
    "bidirectional_transformer = train_model(bidirectional_transformer, X_train_split, y_train_split, X_val, y_val, patience=15)\n",
    "\n",
    "# 训练SVR模型\n",
    "X_train_svr = X_train.cpu().numpy().reshape(-1, time_steps * features.shape[1])\n",
    "y_train_svr = y_train.cpu().numpy()\n",
    "\n",
    "param_grid = {'C': [0.1, 1, 10], 'gamma': [0.001, 0.01]}\n",
    "grid = GridSearchCV(SVR(kernel='rbf'), param_grid, cv=3)\n",
    "grid.fit(X_train_svr, y_train_svr)\n",
    "svr = grid.best_estimator_\n",
    "\n",
    "# 获取各模型预测结果\n",
    "def get_predictions(model, X_test):\n",
    "    if isinstance(model, torch.nn.Module):\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            return model(X_test).cpu().numpy()\n",
    "    else:\n",
    "        X_test_flat = X_test.cpu().numpy().reshape(-1, time_steps * features.shape[1])\n",
    "        return model.predict(X_test_flat)\n",
    "\n",
    "single_pred = get_predictions(single_transformer, X_test)\n",
    "multi_pred = get_predictions(multi_layer_transformer, X_test)\n",
    "bi_pred = get_predictions(bidirectional_transformer, X_test)\n",
    "svr_pred = get_predictions(svr, X_test)\n",
    "\n",
    "# 计算MAPE\n",
    "def calculate_mape(y_true, y_pred):\n",
    "    mask = y_true != 0\n",
    "    if np.sum(mask) == 0:\n",
    "        return float('nan')\n",
    "    return np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100\n",
    "\n",
    "# ---------------------- DQN整合----------------------\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.95\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.model = self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        model = nn.Sequential(\n",
    "            nn.Linear(self.state_size, 24),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(24, 24),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(24, self.action_size)\n",
    "        )\n",
    "        return model\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        act_values = self.model(torch.tensor(state, dtype=torch.float32))\n",
    "        action = np.argmax(act_values.detach().numpy())\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "        return action\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            state = np.array(state).reshape(1, self.state_size)\n",
    "            next_state = np.array(next_state).reshape(1, self.state_size)\n",
    "            \n",
    "            target = reward\n",
    "            if not done:\n",
    "                target = (reward + self.gamma *\n",
    "                          np.amax(self.model(torch.tensor(next_state, dtype=torch.float32)).detach().numpy()))\n",
    "            target_f = self.model(torch.tensor(state, dtype=torch.float32))\n",
    "            target_f[0][action] = target\n",
    "\n",
    "# 训练DQN Agent\n",
    "state_size = 4\n",
    "action_size = 4\n",
    "agent = DQNAgent(state_size, action_size)\n",
    "batch_size = 32\n",
    "episodes = 300\n",
    "\n",
    "for e in range(episodes):\n",
    "    state = np.array([svr_pred[0], single_pred[0], multi_pred[0], bi_pred[0]]).reshape(1, state_size)\n",
    "    for t in range(len(y_test) - 1):\n",
    "        action = agent.act(state)\n",
    "        pred = [svr_pred[t], single_pred[t], multi_pred[t], bi_pred[t]][action]\n",
    "        y_test_value = y_test[t].item()\n",
    "        reward = -np.abs(pred - y_test_value)\n",
    "        next_state = np.array([svr_pred[t + 1], single_pred[t + 1], multi_pred[t + 1], bi_pred[t + 1]]).reshape(1, state_size)\n",
    "        done = t == len(y_test) - 2\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        if len(agent.memory) > batch_size:\n",
    "            agent.replay(batch_size)\n",
    "\n",
    "# RL整合模型\n",
    "class RLAgent(nn.Module):\n",
    "    def __init__(self, num_models):\n",
    "        super(RLAgent, self).__init__()\n",
    "        self.fc1 = nn.Linear(num_models, 16)\n",
    "        self.fc2 = nn.Linear(16, num_models)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.softmax(self.fc2(x), dim=1)\n",
    "        return x\n",
    "\n",
    "# 训练整合模型\n",
    "num_models = 4\n",
    "agent_rl = RLAgent(num_models)  # 重命名为agent_rl，避免与DQNAgent冲突\n",
    "optimizer = optim.Adam(agent_rl.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "epochs = 500\n",
    "all_preds = np.stack([single_pred, multi_pred, bi_pred, svr_pred], axis=1)\n",
    "all_preds_tensor = torch.tensor(all_preds, dtype=torch.float32)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    actions = agent_rl(all_preds_tensor)\n",
    "    selected_preds = (actions * all_preds_tensor).sum(dim=1)\n",
    "    loss = criterion(selected_preds, y_test.cpu())\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# 生成最终DQN预测\n",
    "with torch.no_grad():\n",
    "    actions = agent_rl(all_preds_tensor)\n",
    "    selected_preds = (actions * all_preds_tensor).sum(dim=1).numpy()\n",
    "\n",
    "# 逆标准化\n",
    "def inverse_transform_pred(pred):\n",
    "    dummy = np.zeros((len(pred), features.shape[1]))\n",
    "    dummy[:, y_col] = pred\n",
    "    return scaler.inverse_transform(dummy)[:, y_col]\n",
    "\n",
    "y_test_inverse = inverse_transform_pred(y_test.cpu().numpy())\n",
    "single_pred_inverse = inverse_transform_pred(single_pred)\n",
    "multi_pred_inverse = inverse_transform_pred(multi_pred)\n",
    "bi_pred_inverse = inverse_transform_pred(bi_pred)\n",
    "svr_pred_inverse = inverse_transform_pred(svr_pred)\n",
    "dqn_pred_inverse = inverse_transform_pred(selected_preds) \n",
    "\n",
    "# DQN结果保存\n",
    "dqn_data = {\n",
    "    'date': test_dates,\n",
    "    'DQN_Prediction': dqn_pred_inverse,\n",
    "    'actual_price': y_test_inverse\n",
    "}\n",
    "dqn_prediction_df = pd.DataFrame(dqn_data)\n",
    "dqn_prediction_df['date'] = pd.to_datetime(dqn_prediction_df['date']).dt.strftime(\"%Y-%m-%d\")\n",
    "dqn_file_name = f\"data/{KEYWORD}_agent_integration_prediction_results.csv\"\n",
    "dqn_prediction_df.to_csv(dqn_file_name, index=False)\n",
    "\n",
    "# 评估指标计算\n",
    "rmse_dqn = np.sqrt(mean_squared_error(y_test_inverse, dqn_pred_inverse))\n",
    "mse_dqn = mean_squared_error(y_test_inverse, dqn_pred_inverse)\n",
    "r2_dqn = r2_score(y_test_inverse, dqn_pred_inverse)\n",
    "mae_dqn = mean_absolute_error(y_test_inverse, dqn_pred_inverse)\n",
    "mape_dqn = calculate_mape(y_test_inverse, dqn_pred_inverse)\n",
    "\n",
    "rmse_svr = np.sqrt(mean_squared_error(y_test_inverse, svr_pred_inverse))\n",
    "mse_svr = mean_squared_error(y_test_inverse, svr_pred_inverse)\n",
    "r2_svr = r2_score(y_test_inverse, svr_pred_inverse)\n",
    "mae_svr = mean_absolute_error(y_test_inverse, svr_pred_inverse)\n",
    "mape_svr = calculate_mape(y_test_inverse, svr_pred_inverse)\n",
    "\n",
    "rmse_single = np.sqrt(mean_squared_error(y_test_inverse, single_pred_inverse))\n",
    "mse_single = mean_squared_error(y_test_inverse, single_pred_inverse)\n",
    "r2_single = r2_score(y_test_inverse, single_pred_inverse)\n",
    "mae_single = mean_absolute_error(y_test_inverse, single_pred_inverse)\n",
    "mape_single = calculate_mape(y_test_inverse, single_pred_inverse)\n",
    "\n",
    "rmse_multi = np.sqrt(mean_squared_error(y_test_inverse, multi_pred_inverse))\n",
    "mse_multi = mean_squared_error(y_test_inverse, multi_pred_inverse)\n",
    "r2_multi = r2_score(y_test_inverse, multi_pred_inverse)\n",
    "mae_multi = mean_absolute_error(y_test_inverse, multi_pred_inverse)\n",
    "mape_multi = calculate_mape(y_test_inverse, multi_pred_inverse)\n",
    "\n",
    "rmse_bi = np.sqrt(mean_squared_error(y_test_inverse, bi_pred_inverse))\n",
    "mse_bi = mean_squared_error(y_test_inverse, bi_pred_inverse)\n",
    "r2_bi = r2_score(y_test_inverse, bi_pred_inverse)\n",
    "mae_bi = mean_absolute_error(y_test_inverse, bi_pred_inverse)\n",
    "mape_bi = calculate_mape(y_test_inverse, bi_pred_inverse)\n",
    "\n",
    "# 原有指标输出\n",
    "print(\"\\n=== DQN及基础模型评估指标 ===\")\n",
    "print(f'Our DQN-HTS-EF - RMSE: {rmse_dqn:.4f}, MSE: {mse_dqn:.4f}, R^2: {r2_dqn:.4f}, MAE: {mae_dqn:.4f}, MAPE: {mape_dqn:.4f}%')\n",
    "print(f'SVR Model - RMSE: {rmse_svr:.4f}, MSE: {mse_svr:.4f}, R^2: {r2_svr:.4f}, MAE: {mae_svr:.4f}, MAPE: {mape_svr:.4f}%')\n",
    "print(f'Single-layer Transformer - RMSE: {rmse_single:.4f}, MSE: {mse_single:.4f}, R^2: {r2_single:.4f}, MAE: {mae_single:.4f}, MAPE: {mape_single:.4f}%')\n",
    "print(f'Multi-layer Transformer - RMSE: {rmse_multi:.4f}, MSE: {mse_multi:.4f}, R^2: {r2_multi:.4f}, MAE: {mae_multi:.4f}, MAPE: {mape_multi:.4f}%')\n",
    "print(f'Bidirectional Transformer - RMSE: {rmse_bi:.4f}, MSE: {mse_bi:.4f}, R^2: {r2_bi:.4f}, MAE: {mae_bi:.4f}, MAPE: {mape_bi:.4f}%')\n",
    "\n",
    "\n",
    "# ---------------------- MCS集成 ----------------------\n",
    "print(\"\\n=== MCS集成计算 ===\")\n",
    "# 1. MCS核心函数\n",
    "def calculate_timewise_loss(y_true, y_pred, loss_type='mae'):\n",
    "    \"\"\"计算逐时间点损失\"\"\"\n",
    "    if loss_type == 'mae':\n",
    "        return np.abs(y_true - y_pred)\n",
    "    elif loss_type == 'mse':\n",
    "        return (y_true - y_pred) ** 2\n",
    "    raise ValueError(\"loss_type must be 'mae' or 'mse'\")\n",
    "\n",
    "def bootstrap_pvalue(loss_a, loss_b, n_bootstrap=1000, alpha=0.05):\n",
    "    \"\"\"自助法计算模型成对比较的p值\"\"\"\n",
    "    n_samples = len(loss_a)\n",
    "    diff_obs = loss_a - loss_b\n",
    "    null_distribution = []\n",
    "    for _ in range(n_bootstrap):\n",
    "        idx = np.random.choice(n_samples, size=n_samples, replace=True)\n",
    "        diff_boot = diff_obs[idx]\n",
    "        null_distribution.append(np.mean(diff_boot))\n",
    "    obs_mean = np.mean(diff_obs)\n",
    "    p_value = np.sum(np.abs(null_distribution) >= np.abs(obs_mean)) / n_bootstrap\n",
    "    return p_value, (p_value < alpha)\n",
    "\n",
    "def run_mcs(loss_matrix, model_names, n_bootstrap=1000, alpha=0.05):\n",
    "    \"\"\"运行MCS检验，筛选模型信度集\"\"\"\n",
    "    current_candidates = list(range(len(model_names)))\n",
    "    elimination_order = []\n",
    "    p_value_history = []\n",
    "\n",
    "    while len(current_candidates) > 1:\n",
    "        n_candidates = len(current_candidates)\n",
    "        p_matrix = np.ones((n_candidates, n_candidates))\n",
    "        \n",
    "        # 计算所有成对模型的p值\n",
    "        for i in range(n_candidates):\n",
    "            for j in range(i+1, n_candidates):\n",
    "                loss_i = loss_matrix[:, current_candidates[i]]\n",
    "                loss_j = loss_matrix[:, current_candidates[j]]\n",
    "                p_val, _ = bootstrap_pvalue(loss_i, loss_j, n_bootstrap, alpha)\n",
    "                p_matrix[i, j] = p_val\n",
    "                p_matrix[j, i] = p_val\n",
    "        \n",
    "        # 剔除最差模型（最小p值最小的模型）\n",
    "        min_p_per_model = np.min(p_matrix, axis=1)\n",
    "        worst_idx_in_curr = np.argmin(min_p_per_model)\n",
    "        worst_model_idx = current_candidates[worst_idx_in_curr]\n",
    "        worst_model_name = model_names[worst_model_idx]\n",
    "        worst_p = min_p_per_model[worst_idx_in_curr]\n",
    "        \n",
    "        elimination_order.append(worst_model_name)\n",
    "        p_value_history.append(worst_p)\n",
    "        current_candidates.pop(worst_idx_in_curr)\n",
    "        print(f\"MCS剔除模型：{worst_model_name}，p值：{worst_p:.4f}（<{alpha}为显著差异）\")\n",
    "\n",
    "    # 最终信度集\n",
    "    mcs_set = [model_names[idx] for idx in current_candidates]\n",
    "    print(f\"MCS模型信度集：{mcs_set}\")\n",
    "    return mcs_set, elimination_order, p_value_history\n",
    "\n",
    "# 2. 基于原有基础模型预测构建MCS输入\n",
    "models_mcs = {\n",
    "    'Single-Transformer': single_pred_inverse,\n",
    "    'Multi-Transformer': multi_pred_inverse,\n",
    "    'Bi-Transformer': bi_pred_inverse,\n",
    "    'SVR': svr_pred_inverse\n",
    "}\n",
    "model_names_mcs = list(models_mcs.keys())\n",
    "\n",
    "# 3. 计算逐时间点损失矩阵\n",
    "timewise_loss = {}\n",
    "for name, pred in models_mcs.items():\n",
    "    timewise_loss[name] = calculate_timewise_loss(y_test_inverse, pred, loss_type='mae')\n",
    "loss_matrix = np.column_stack([timewise_loss[name] for name in model_names_mcs])\n",
    "\n",
    "# 4. 运行MCS\n",
    "mcs_set, elimination_order, p_value_history = run_mcs(\n",
    "    loss_matrix=loss_matrix,\n",
    "    model_names=model_names_mcs,\n",
    "    n_bootstrap=1000,\n",
    "    alpha=0.05\n",
    ")\n",
    "\n",
    "# 5. 生成MCS组合预测\n",
    "def mcs_ensemble_pred(mcs_set, models):\n",
    "    \"\"\"MCS等权重组合预测\"\"\"\n",
    "    mcs_preds = [models[name] for name in mcs_set]\n",
    "    weights = np.ones(len(mcs_set)) / len(mcs_set)  # 等权重\n",
    "    mcs_pred_inverse = np.average(mcs_preds, weights=weights, axis=0)\n",
    "    # 打印MCS权重\n",
    "    print(f\"\\nMCS组合权重（等权重）：\")\n",
    "    for name, w in zip(mcs_set, weights):\n",
    "        print(f\"  {name}: {w:.4f}\")\n",
    "    return mcs_pred_inverse, weights\n",
    "\n",
    "mcs_pred_inverse, mcs_weights = mcs_ensemble_pred(mcs_set, models_mcs)\n",
    "\n",
    "# 6. MCS评估指标\n",
    "rmse_mcs = np.sqrt(mean_squared_error(y_test_inverse, mcs_pred_inverse))\n",
    "mse_mcs = mean_squared_error(y_test_inverse, mcs_pred_inverse)\n",
    "r2_mcs = r2_score(y_test_inverse, mcs_pred_inverse)\n",
    "mae_mcs = mean_absolute_error(y_test_inverse, mcs_pred_inverse)\n",
    "mape_mcs = calculate_mape(y_test_inverse, mcs_pred_inverse)\n",
    "\n",
    "print(f\"\\nMCS集成模型评估指标：\")\n",
    "print(f'MCS-Ensemble - RMSE: {rmse_mcs:.4f}, MSE: {mse_mcs:.4f}, R^2: {r2_mcs:.4f}, MAE: {mae_mcs:.4f}, MAPE: {mape_mcs:.4f}%')\n",
    "\n",
    "\n",
    "# ---------------------- Diebold-Mariano（DM）检验（MCS vs DQN） ----------------------\n",
    "print(\"\\n=== DM检验（MCS vs DQN） ===\")\n",
    "def diebold_mariano_test(y_true, pred_base, pred_comp, loss_type='mse'):\n",
    "    \"\"\"\n",
    "    DM检验：对比基准模型（MCS）和对比模型（DQN）\n",
    "    loss_type: 用MSE（与原有指标一致，关注大误差）\n",
    "    \"\"\"\n",
    "    # 计算原始尺度损失\n",
    "    if loss_type == 'mse':\n",
    "        loss_base = (y_true - pred_base) ** 2  # MCS损失\n",
    "        loss_comp = (y_true - pred_comp) ** 2  # DQN损失\n",
    "    elif loss_type == 'mae':\n",
    "        loss_base = np.abs(y_true - pred_base)\n",
    "        loss_comp = np.abs(y_true - pred_comp)\n",
    "    else:\n",
    "        raise ValueError(\"loss_type仅支持'mse'或'mae'\")\n",
    "    \n",
    "    # 损失差异（DQN - MCS）\n",
    "    loss_diff = loss_comp - loss_base\n",
    "    n = len(loss_diff)\n",
    "    loss_diff_mean = np.mean(loss_diff)\n",
    "    \n",
    "    # Newey-West调整（处理时间序列自相关）\n",
    "    gamma0 = np.var(loss_diff)  # 滞后0阶自协方差\n",
    "    gamma1 = np.cov(loss_diff[:-1], loss_diff[1:])[0, 1]  # 滞后1阶自协方差\n",
    "    se_dm = np.sqrt((gamma0 + 2 * gamma1) / n)  # 稳健标准误\n",
    "    \n",
    "    # DM统计量和p值\n",
    "    dm_stat = loss_diff_mean / se_dm if se_dm != 0 else 0.0\n",
    "    t_result = stats.ttest_1samp(loss_diff, 0)  # 双尾检验\n",
    "    p_value = t_result.pvalue\n",
    "    \n",
    "    return dm_stat, p_value, loss_diff_mean\n",
    "\n",
    "# 执行DM检验（MCS为基准，DQN为对比）\n",
    "dm_stat, dm_p_value, dm_loss_diff_mean = diebold_mariano_test(\n",
    "    y_true=y_test_inverse,\n",
    "    pred_base=mcs_pred_inverse,  # 基准：MCS集成\n",
    "    pred_comp=dqn_pred_inverse,  # 对比：原有DQN\n",
    "    loss_type='mse'  # 与原有评估指标一致\n",
    ")\n",
    "\n",
    "# 打印DM结果\n",
    "dm_result = pd.DataFrame({\n",
    "    '基准模型': ['MCS-Ensemble'],\n",
    "    '对比模型': ['DQN-HTS-EF'],\n",
    "    'DM统计量': [round(dm_stat, 4)],\n",
    "    'P值': [round(dm_p_value, 4)],\n",
    "    '损失差异均值（DQN-MCS）': [round(dm_loss_diff_mean, 4)],\n",
    "    '差异显著性（p<0.05）': ['是' if dm_p_value < 0.05 else '否'],\n",
    "    '结论': [\n",
    "        'MCS显著更优' if (dm_loss_diff_mean > 0 and dm_p_value < 0.05) else\n",
    "        'DQN显著更优' if (dm_loss_diff_mean < 0 and dm_p_value < 0.05) else\n",
    "        '无显著差异'\n",
    "    ]\n",
    "})\n",
    "print(\"\\nDM检验结果：\")\n",
    "print(dm_result.to_string(index=False))\n",
    "\n",
    "\n",
    "# ---------------------- 可视化 ----------------------\n",
    "print(\"\\n=== 生成MCS vs DQN对比可视化 ===\")\n",
    "plt.figure(figsize=(14, 7))  \n",
    "# 原有曲线（不变）\n",
    "plt.plot(test_dates, y_test_inverse, '-', color='black', linewidth=2, label='Actual Price', alpha=0.8)\n",
    "plt.plot(test_dates, single_pred_inverse, '--', color='tab:blue', linewidth=1.5, label='Single-layer Transformer', alpha=0.7)\n",
    "plt.plot(test_dates, multi_pred_inverse, '--', color='tab:green', linewidth=1.5, label='Multi-layer Transformer', alpha=0.7)\n",
    "plt.plot(test_dates, bi_pred_inverse, '--', color='tab:red', linewidth=1.5, label='Bidirectional Transformer', alpha=0.7)\n",
    "plt.plot(test_dates, svr_pred_inverse, '-.', color='tab:orange', linewidth=1.5, label='SVR', alpha=0.7)\n",
    "plt.plot(test_dates, dqn_pred_inverse, '-', color='tab:purple', linewidth=2, label='Our DQN-HTS-EF', alpha=0.9)\n",
    "# 新增MCS曲线\n",
    "plt.plot(test_dates, mcs_pred_inverse, '-', color='tab:cyan', linewidth=2, label='MCS-Ensemble', alpha=0.9)\n",
    "\n",
    "# 原有日期处理（不变）\n",
    "unique_months = []\n",
    "unique_indices = []\n",
    "month_set = set()\n",
    "for index, date in enumerate(pd.to_datetime(test_dates)):\n",
    "    month_key = date.strftime('%Y-%m')\n",
    "    if month_key not in month_set:\n",
    "        unique_months.append(date.strftime('%Y-%m'))\n",
    "        unique_indices.append(index)\n",
    "        month_set.add(month_key)\n",
    "\n",
    "plt.xticks(unique_indices, unique_months, rotation=45)\n",
    "plt.xlabel('Date', fontsize=12)\n",
    "plt.ylabel('Price', fontsize=12)\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.title(f'Price Prediction: DQN vs MCS ({KEYWORD})', fontsize=14)  \n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ---------------------- MCS和DM结果保存 ----------------------\n",
    "print(\"\\n=== 保存MCS和DM结果 ===\")\n",
    "# MCS结果保存\n",
    "mcs_result_data = {\n",
    "    'MCS信度集': [', '.join(mcs_set)],\n",
    "    '剔除顺序': [', '.join(elimination_order)],\n",
    "    '剔除时p值': [', '.join([f'{p:.4f}' for p in p_value_history])],\n",
    "    'MCS权重': [', '.join([f'{name}:{w:.4f}' for name, w in zip(mcs_set, mcs_weights)])]\n",
    "}\n",
    "mcs_result_df = pd.DataFrame(mcs_result_data)\n",
    "mcs_result_df.to_csv(f\"data/{KEYWORD}_mcs_results.csv\", index=False)\n",
    "\n",
    "# DM结果保存\n",
    "dm_result.to_csv(f\"data/{KEYWORD}_dm_test_results.csv\", index=False)\n",
    "\n",
    "# 汇总指标保存（原有+MCS）\n",
    "summary_metrics = pd.DataFrame([\n",
    "    {'Model': 'DQN-HTS-EF', 'RMSE': rmse_dqn, 'MSE': mse_dqn, 'R²': r2_dqn, 'MAE': mae_dqn, 'MAPE(%)': mape_dqn},\n",
    "    {'Model': 'MCS-Ensemble', 'RMSE': rmse_mcs, 'MSE': mse_mcs, 'R²': r2_mcs, 'MAE': mae_mcs, 'MAPE(%)': mape_mcs},\n",
    "    {'Model': 'Single-Transformer', 'RMSE': rmse_single, 'MSE': mse_single, 'R²': r2_single, 'MAE': mae_single, 'MAPE(%)': mape_single},\n",
    "    {'Model': 'Multi-Transformer', 'RMSE': rmse_multi, 'MSE': mse_multi, 'R²': r2_multi, 'MAE': mae_multi, 'MAPE(%)': mape_multi},\n",
    "    {'Model': 'Bi-Transformer', 'RMSE': rmse_bi, 'MSE': mse_bi, 'R²': r2_bi, 'MAE': mae_bi, 'MAPE(%)': mape_bi},\n",
    "    {'Model': 'SVR', 'RMSE': rmse_svr, 'MSE': mse_svr, 'R²': r2_svr, 'MAE': mae_svr, 'MAPE(%)': mape_svr}\n",
    "])\n",
    "summary_metrics.to_csv(f\"data/{KEYWORD}_summary_metrics.csv\", index=False)\n",
    "\n",
    "print(f\"MCS结果保存至：data/{KEYWORD}_mcs_results.csv\")\n",
    "print(f\"DM检验结果保存至：data/{KEYWORD}_dm_test_results.csv\")\n",
    "print(f\"汇总指标保存至：data/{KEYWORD}_summary_metrics.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
